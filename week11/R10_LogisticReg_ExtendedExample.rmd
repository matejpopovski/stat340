---
title: "Logistic Regression - Extended Pima Example"
author: "Brian Powers"
date: "2023-04-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(MASS)
head(Pima.te)
Pima.te$glu_bin <- (round(Pima.te$glu/10))*10
Pima.te$typeTF <- as.numeric(Pima.te$type=="Yes")
Pima.glm <- glm(typeTF ~ glu, data=Pima.te, family="binomial")

aggr.props <- aggregate(typeTF ~ glu_bin, data=Pima.te, FUN=mean)

xs <- seq(60,200,10)
zs <- predict(Pima.glm, newdata=data.frame(glu=xs))
ps <- exp(zs)/(1+exp(zs))

plot(x=Pima.te$glu, y=Pima.te$typeTF, pch=16, col=rgb(0,0,1,.2), xlim=c(50,200))
#points(aggr.props, ylim=c(0,1))
lines(x=xs,y=ps, col="red")
text(x=xs-2, y=as.vector(aggr.props$typeTF*.8+.5*.2), label=round(aggr.props$typeTF,2))
segments(x0=xs-5, aggr.props$typeTF, xs+5,aggr.props$typeTF)
abline(v=seq(55,195,10), col="gray")
```

Model summary
```{r}
summary(Pima.glm)

```
The log(odds ratio)=0.042421
The odds ratio = exp(0.042421)
```{r}
exp(0.042421)
```
odds ratio vs risk ratio

Predict probability for 200 vs 201
```{r}
predict(Pima.glm, type="response", newdata=data.frame(glu=c(200,201)))

#The risk ratio is
0.9295508 / 0.9267217

#The Odds ratio is
(0.9295508/(1-0.9295508)) / (0.9267217/(1-0.9267217))
```

Compare 100 to 101
```{r}
predict(Pima.glm, type="response", newdata=data.frame(glu=c(100,101)))

#The risk ratio is
0.1594550 / 0.1538511 

#The Odds ratio is
(0.1594550/(1-0.1594550)) / (0.1538511 /(1-0.1538511 ))
```





```{r}
#log likelihood calculation
#Let's compare the true Y values and the predicted Y values

y <- Pima.te$typeTF

y.hat <- predict(Pima.glm, type="response")

head(data.frame(y,y.hat))
```


The likelihood is the product of predicted probabilities (of the actual label)
For actual 1s, we use y.hat
For actual 0s we use 1-y.hat
```{r}
L <- prod(y * y.hat + (1-y)*(1-y.hat))
L
```

Maximizing likelihood is equivalent to maximising the log of likelihood, and because logarithms turn products into sums, the log likelihood is easier to work with mathematically. Because likelihood < 1, log likelihood is going to be negative.
```{r}
log(L)

l <- sum(log(y * y.hat + (1-y)*(1-y.hat)))
l
```
The closer to zero the log likelihood is, the better; The closer to -infinity, the "worse". 
The residual deviance is defined as -2 times the log likelihood. It turns a big negative log likelihood into a large positive number (easier to interpret)

```{r}
#residual deviance
-2*l
```

# Prediction
Suppose someone has glucose level 150; what is the estimated probability of diabetes?

```{r}
#Predict log odds
predict(Pima.glm, newdata=data.frame(glu=150))

#Predics probability
predict(Pima.glm, newdata=data.frame(glu=150), type="response")

#calculation
log.odds.150 <- sum(Pima.glm$coefficients * c(1, 150))

#Estimated probability = e^log-odds / (1+e^log-odds)
#or
#                      = 1/ (1 + e^ -logodds)
exp(log.odds.150) / (1+ exp(log.odds.150))
#or
1 / (1+ exp(-log.odds.150))

```

## The null model

The null model would be what we compare our "better" model to - it is a logistic model that does not have any predictors. Just like the null model for linear regression predicts $\hat{y}=\bar{y}$ the null model predicts $\hat{y}=$ the proportion of 1s in the data set.


```{r}
pima.glm.null <- glm(typeTF ~ 1, data=Pima.te, family="binomial")
summary(pima.glm.null)

log.odds <- pima.glm.null$coefficients[1]
exp(log.odds) / (1+ exp(log.odds))


mean(Pima.te$typeTF)
```
Let's just double check that. We can calculate the coefficient manually
```{r}
#What is the proportion of 1s in the data?
(prop1 <- mean(Pima.te$typeTF))

#convert this to a log-odds
log(prop1/(1-prop1))
```
This is the same as the constant in the null model.



## A Logistic model with a categorical predictor

```{r}
#Let's use age as a predictor
#Only for age > 50
Pima.te$age50 <- as.numeric(Pima.te$age > 50)

diabetes.aggr <- aggregate(typeTF ~ age50, data=Pima.te, FUN=mean)
diabetes.aggr
```
Among those 50 or younger, proportion with diabetes is .3129032
among those 51+ it is .545454
```{r}
#Log-odds for the two groups
odds <- diabetes.aggr$typeTF / (1-diabetes.aggr$typeTF)
odds
logodds <- log(odds)
logodds
diff(logodds)
```
Notice the log-odds are -.7865 and .1823 respectively. If we were to build a model we would say that
$$ \hat{LO} = -.7865812 + .9689 \times age_{>50}$$


```{r}
Pima50 <- glm(typeTF ~ age50, data=Pima.te, family="binomial")
summary(Pima50)

```


Interpret the slope
```{r}
Pima.glm$coefficients[2]
exp(Pima.glm$coefficients[2])
```
Odd ratio for a 1 unit increase in blood glucose is 1.0433
so if we hold all other variables constant and increase blood glucose by 1 unit, we expect an increase in the Odds by 4.33%



```{r}
#Prob of diabetes:
p.hat <- mean(Pima.te$typeTF)
#Odds of diabetes
odds.1 <- p.hat/(1-p.hat)
#Logodds diabetes
log.odds.1 <- log(odds.1)
log.odds.1

#Null model
summary(glm(typeTF ~ 1, data=Pima.te, family="binomial"))
```

```{r}
#null deviance calculation

#log likelihood = sum [y * y.hat + (1-y)*(1-yhat)]

#deviance = -2*log likelihood

n <- nrow(Pima.te)
m <- sum(Pima.te$typeTF)
-2*(m*log(m/n) + (n-m)*log((n-m)/n))
```




## Results of logistic regression: confusion matrix

See https://en.wikipedia.org/wiki/Confusion_matrix

```{r}

pima_logit <- glm(typeTF ~ glu, data=Pima.te, family="binomial")

confusion.table <- table(Pima.te$typeTF, factor(predict(pima_logit, type="response") >= .5, levels=c(FALSE, TRUE)))

colnames(confusion.table) <- c("Predict 0","Predict 1")
rownames(confusion.table) <- c("No diabetes","Yes diabetes")
confusion.table

#confusion.table
TP <- confusion.table[2,2]
TN <- confusion.table[1,1]
FP <- confusion.table[1,2]
FN <- confusion.table[2,1]
```
Statistics for a predictor:

Accuracy of a classifier - the total # correct predictions / total predictions

```{r}
(accuracy <- (TP+TN)/(TP+TN+FP+FN))  
  
```
Sensitivity (recall, hit rate, true positive rate) - what proportion of actual positive cases are classified as positive
```{r}
(sensitivity <- TP / (TP+FN))
```
Specificity (selectivity, true negative rate) - what proportion of actual negative cases are classified as negative
```{r}
(specificity <- TN / (TN+FP))
```
Precision (positive predictive value) - what proportion of positive predictions are correct
```{r}
(ppv <- TP/(TP+FP))
```

Negative Predictive Value (NPV) - what proportion of negative predictions are correct
```{r}
(npv <- TN/(TN+FN))
```
What was the Type 1 error rate? The False positive rate
```{r}
FP / (FP + TN)
```


By adjusting the positive prediction threshold we can improve/worsen statistics such as accuracy, specificity, sensitivity, etc.

```{r}
pima_logit <- glm(typeTF ~ glu, data=Pima.te, family="binomial")
acc <- 0
thresh <- seq(0.01, 0.99, .01)
for(i in 1:length(thresh)){
  confusion.table <- table(Pima.te$typeTF, factor(predict(pima_logit, type="response")>= thresh[i], levels=c(FALSE, TRUE)))
  colnames(confusion.table) <- c("Predict 0","Predict 1")
  rownames(confusion.table) <- c("No diabetes","Yes diabetes")
  #confusion.table
  TP <- confusion.table[2,2]
  TN <- confusion.table[1,1]
  FP <- confusion.table[1,2]
  FN <- confusion.table[2,1]
  
  acc[i] = (TP+TN)/(TP+FP+TN+FN)
  
}
plot(thresh, acc, type="l", main="Accuracy by threshold")
```

In particular, we can look at the relationship between True positive rate and False Positive Rate (what proportion of actual negatives are incorrectly predicted to be positive) (FP / (FP+TN))
```{r}
TPR <- 0; FPR <- 0; 
thresh <- seq(0, 1, .001)
for(i in 1:length(thresh)){
  confusion.table <- table(Pima.te$typeTF, factor(predict(pima_logit, type="response")>= thresh[i], levels=c(FALSE, TRUE)))
  colnames(confusion.table) <- c("Predict 0","Predict 1")
  rownames(confusion.table) <- c("No diabetes","Yes diabetes")
  #confusion.table
  TP <- confusion.table[2,2]
  TN <- confusion.table[1,1]
  FP <- confusion.table[1,2]
  FN <- confusion.table[2,1]
  
  TPR[i] <- TP/(TP+FN)
  FPR[i] <- FP/(FP+TN)
}
plot(thresh, TPR, type="n", main="TPR and FPR by threshold", xlim=c(0,1), ylim=c(0,1))
lines(thresh, TPR, lty=1)
lines(thresh, FPR, lty=2)
legend(x=.8, y=1, legend = c("TPR", "FPR"), lty=c(1,2))
```

To put things into the language of hypothesis testing we could ask this: What threshold would we use to achieve a 5% significance level (false positive rate) and what would the power of the classifier be?
```{r}
optimalThresh <- min(which(FPR<=0.05))
thresh[optimalThresh]
as.numeric(FPR[optimalThresh])
as.numeric(TPR[optimalThresh])
```



A scatterplot of these rates is called the ROC curve. We start at (0,0) and go to (1,1)
```{r}
plot(c(1,FPR,0),c(1, TPR,0), type="s", xlab="False Positive Rate", ylab="True Positive Rate")

```

The PRROC package produces pretty ROC curves. 

```{r}
#install.packages("PRROC")
library(PRROC)

PRROC_obj <- roc.curve(scores.class0 = predict(pima_logit, type="response"), 
                       weights.class0=Pima.te$typeTF,
                       curve=TRUE)
plot(PRROC_obj)
```

The area under the curve (AUC) is a common metric measuring how good your classifier is. An AUC = 0.5 (where the ROC curve is a diagonal line) is a "dumb" predictor. AUC closer to 1 represents a discerning "good" predictor.






```{r, eval=FALSE, echo=FALSE}
### Work in progress
library(pROC)
library(tidyverse)

test_prob = predict(Pima.glm, newdata = pima.test, type = "response")
ggroc(test_roc <- roc(pima.test$typeTF~test_prob,print.auc=T),legacy.axes=T) + 
  labs(title="ROC curve",subtitle=glue::glue("AUC: {round(test_roc$auc,1)}"),
       x="False positive rate",y="True positive rate") + 
  scale_x_continuous(expand=c(0,0)) + scale_y_continuous(expand=c(0,0)) + 
  geom_abline() + coord_equal()
```

