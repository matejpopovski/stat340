---
title:  "STAT340 Discussion 4: Hypothesis Testing"
output: html_document
author: "Matej Popovski, Alex Holland, Brock Lunstrum, Zoe Weinstein"
---

<style>
table{width:50%!important;margin-left:auto!important;margin-right:auto!important;}
ol[style*="decimal"]>li{margin-top:40px!important;}
</style>

<br/>


## XKCD comic

<center><a href="https://xkcd.com/539/"><img id="comic" src="https://imgs.xkcd.com/comics/boyfriend.png" title="... okay, but because you said that, we're breaking up."></a></center>


### Linear regression coefficient testing

Consider this problem in the context of linear regression.

__Note:__ we will return to linear regression later this semester. For now, your background from STAT240 will be plenty. Also, in the future you will see other techniques for this problem beyond permutation tests. Still, permutation tests are extremely versatile and are well suited to hypothesis testing, so we will continue using them for now.


a) Inspect the built-in example data frame `mtcars` that's loaded with every R session. **Briefly read the help page** by running `?mtcars` if you are unfamiliar with the dataset.
   
There are many variables in this data set, but today we will just focus on **`mpg` (the response variable) and `drat` (a predictor variable)**.
Make sure you know what these two variables refer to in real life (see the help page).
Create a new data frame called `mtcars.small` with just these two columns and print this new data frame.

```{r}
?mtcars
mtcars
data(mtcars)
### TODO: uncomment the below and complete.

mtcars.small <- mtcars[,c("mpg", "drat")]
mtcars.small
```

b) Below, we've performed a simple linear regression, showing the coefficients. **Take note of the coefficient estimate for the slope $\beta_1$**, which represents the strength of the linear relationship between the two variables.
__Note:__ this measure is NOT the same as _significance_; [see this](https://en.wikipedia.org/wiki/Statistical_significance#Effect_size).
   
One way to estimate the _significance_ of $\beta_1$ (and the one that is most commonly taught at the undergraduate level) is to use the linear regression model to derive a [$t$-statistic test](https://www.real-statistics.com/regression/hypothesis-testing-significance-regression-line-slope/) for the coefficients and then compare the estimates to the null hypothesis that $\beta_1=0$, i.e., the hypothesis that there is no linear relationship between the variables.
This would be a *parametric* test, because it relies on specific assumptions about the distribution of the responses in our linear regression model.

You can see from the output below the slope has an estimate of $7.678$ and a standard error of $1.507$.
The $t$-statistic is then calculated as $(7.678-0)/1.507=5.096$, with $n-2=30$ degrees of freedom, where $n$ is the number of observations.
We can then compute the $p$-value using `2*(1-pt(5.096,30))` and confirm **it matches the table**.
Don't worry too much if you don't fully understand this.
Do note, however, that the process is very similar to how you would perform an ordinary $t$-test like you saw in STAT240.

```{r}
# #TODO: uncomment code below and take note of slope estimate.
# #Fist we run lm with mpg regressed on drat,
data=mtcars.small #means use columns from this data frame,
# #then  extract coefficients from model
# 
lm(formula = mpg ~ drat, data = mtcars.small)
summary(lm(mpg ~ drat, data=mtcars.small))$coefficients
2*(1-pt(5.096042,30))
# 
# # Code for extracting the coefficient beta_1
output <- lm( mpg ~ drat, data=mtcars.small);
output$coefficients[2]
```
c) Another way you can estimate the significance of $\beta_1$ under the null hypothesis of $\beta_1=0$ is by (you guessed it!) permuting the data!
If indeed $\beta_1=0$ (i.e., in the language of our data set, there is no linear relation between `mpg` and `drat`), then permuting the $x$ coordinates (i.e., the independent variables) so they get assigned randomly to the $y$ coordinates (i.e., the responses) should have no effect.
This is because $\beta_1=0$ implies the relationship between $y$ and $x$ is actually a flat line.
   
Write a function that performs $M$ iterations of the following:
   
   i. Randomly permute the $x$ coordinates (`drat` values).
   ii. Use `lm` to run linear regression, regressing the $y$ coordinates (`mpg` values) against the permuted $x$ coordinates.
   iii. Save the estimate for the slope.
   
   Run your function with at least $M=1000$ (can you get it to run with $M=10,000$ or even higher?), and compare the distribution of your $M$ estimated slopes against the the slope estimate of our original observed data set.
   Use these $M$ replicates to obtain a $p$-value associated with our null hypothesis that there is no linear relation between `mpg` and `drat` (i.e., $H_0 : \beta_1=0$).
   
   - __Note:__ If the method in part (b) is accurate and the $p$-value is on the order of $10^{-5}$, you may need $10^5=100,000$ iterations to see a single case where the permuted slope is equal to or greater than the actual slope (in absolute value).
   Thus, if you run a lower $N$ and don't see a single case, that's okay, it just means the $p$-value is so small that your simulation size cannot detect it.
   This is what happened in our COVID vaccine example in lecture!

```{r}
### TODO: complete c here.
# Function to perform permutation test with output for comparison
  permutation_test_with_output <- function(data, M) {
  # Store the original model for comparison
  original_model <- lm(mpg ~ drat, data = data)
  original_beta1 <- coef(original_model)["drat"]
  
  # Vector to store permuted beta1 coefficients
  permuted_beta1 <- numeric(M)
  
  # Perform M permutations
  for (i in 1:M) {
    # Permute the drat values
    permuted_drat <- sample(data$drat)
    
    # Fit the model with permuted drat values
    permuted_model <- lm(mpg ~ permuted_drat, data = data)
    
    # Store the beta1 coefficient from the permuted model
    permuted_beta1[i] <- coef(permuted_model)["permuted_drat"]
  }
  
  # Calculate the p-value (two-sided test)
  p_value <- mean(abs(permuted_beta1) >= abs(original_beta1))
  
  # Output the results
  cat("Original β1 (from real data):", original_beta1, "\n\n")
  
  # Summary statistics of permuted beta1 coefficients
  cat("Summary of Permuted β1 Coefficients:\n")
  print(summary(permuted_beta1))
  
  # Plot the distribution of permuted beta1 coefficients
  hist(permuted_beta1, main="Distribution of Permuted β1 Coefficients",   xlab="Permuted β1", col="lightblue", breaks=20)
  abline(v = original_beta1, col = "red", lwd = 2, lty = 2)
  legend("topright", legend = c("Original β1"), col = "red", lty = 2, lwd = 2)

# Output p-value
  cat("\nP-value from permutation test:", p_value, "\n")
  
  # Return the permuted beta1 values (optional, for further analysis)
  return(list(permuted_beta1 = permuted_beta1, original_beta1 = original_beta1, p_value = p_value))
}

# Use the function with M=1000 and M=10000
set.seed(123)  # For reproducibility

mtcars.small <- mtcars[,c("mpg", "drat")]
data(mtcars.small)

# Run permutation test with M=1000
cat("Permutation test with M=1000 iterations:\n")
result_1000 <- permutation_test_with_output(mtcars, M=1000)

# Run permutation test with M=10000
cat("\nPermutation test with M=10000 iterations:\n")
result_10000 <- permutation_test_with_output(mtcars, M=10000)


# Run permutation test with M=10000
cat("\nPermutation test with M=10000 iterations:\n")
result_100000 <- permutation_test_with_output(mtcars, M=100000)


```

