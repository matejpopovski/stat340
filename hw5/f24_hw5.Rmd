---
title: "Homework 5"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=T,eval=T,message=F,warning=F,fig.align='center')
library(tidyverse)
```


## Problem 1. A Data Scientist Referees Ping Pong <small>(8pts total)</small>

The game is Ping Pong. Players grab their paddles and hit the ping pong ball back and forth scoring points one at a time. The game continues until one player reaches 21 points, and at that point the game ends unless the point difference is less than 2. If it is less than 2 the game continues until one player wins by 2. 

Suppose Athena and Bacchus play and Bacchus wins 21 to 15. Bacchus is super excited but Athena says that they should have a rematch, because she's sure that Bacchus is not better than her, it was just a fluke. 

Time for a Data Scientist to settle this dispute. We must consider two hypotheses. The null hypothesis is that they are equally skilled - thus for each point scored the probability it goes to the ultimate winner is $0.50$ (this is basically what Athena is claiming). The alternative is that Bacchus truly is more skilled, and the probability is greater than $0.50$ (the winner actually has more skill, and doesn't win purely by chance).

Create a Monte Carlo simulation of a game. Use the point difference at game end as the test statistic. 

a. (4 points) Create a function called `playPingPong` which simulates a single game of Ping Pong with equally skilled players. Remember the logic : points are given to players A or B with equal chance, and that continues until (1) the max score >= 21 and (2) the difference between scores >=2. Have the function return the point difference.

```{r}
playPingPong <- function(scoreTrigger = 21, winMargin=2, probPlayerA=0.5){
  #initialize a score vector to be 0,0
  scoreA <- 0
  scoreB <- 0
  #repeat while the winning condition has not been reached
  #i.e. repeat while max score < 21 or point difference < 2
  while ((max(scoreA, scoreB) < scoreTrigger) || (abs(scoreA - scoreB) < winMargin)) {
    # Assign a point to Player A or Player B based on probPlayerA
    if (runif(1) < probPlayerA) {
      scoreA <- scoreA + 1
    } else {
      scoreB <- scoreB + 1
    }
  }
    #Assign a point to player 1 or 2 randomly

  #return the point difference
  return(abs(scoreA - scoreB))
}
point_difference <- playPingPong()
cat("The point difference is:", point_difference, "\n")
```

b. (2 points) Perform the Monte Carlo test; simulate 1000 (or more) games with equally skilled players. Look at the distribution of point differences, and compare the observed point difference to this distribution. What is the *p*-value of the observed point difference.

```{r}
# Set seed for reproducibility
set.seed(123)

# Number of simulations
num_simulations <- 10000

# Observed point difference from the actual game
observed_diff <- 21 - 15  # 6 points

# Simulate multiple games and record point differences
simulated_diffs <- replicate(num_simulations, playPingPong())

# Plot the distribution of simulated point differences
hist(simulated_diffs, breaks = 30, main = "Distribution of Point Differences",
     xlab = "Point Difference", col = "lightblue", border = "black")

# Calculate the p-value
# Since Bacchus won by 6 points, we consider point differences >=6
# under the alternative that Bacchus is better (one-sided test)
p_value <- mean(simulated_diffs >= observed_diff)

# Display the p-value
cat("P-value:", p_value, "\n")

```


c. (2 points) How do you conclude? Is this one game sufficient evidence that Bacchus is the superior Ping Pong Player?

> Based on the Monte Carlo simulation, the p-value of 0.4035 indicates that there is insufficient evidence to conclude that Bacchus is a superior Ping Pong player. We fail to reject the null hypothesis, meaning that the observed point difference of 6 could easily occur by chance if both players are equally skilled. Therefore, this one game is not sufficient to prove that Bacchus is better than Athena, and more games would be needed for a more reliable assessment.


## Problem 2: Quality or Quantity? <small>(6pts total)</small> 

Marcio Ranchello (fictional) is a prolific architect who has won many accolades. For example, in the ranking of "Best 10 designs of 2023", 4 of the 10 designs are from Marcio Ranchello. The authors of the ranking report suggest that this is evidence of his greatness. However, you notice that among the 150 buildings considered in the rankings, 30 of them were designed by Marcio. Indeed, Marcio leads a big architecture firm that has been extremely active in designing new buildings.

What do you think? Is the ranking evidence of the quality of his work, or a consequence of the quantity of his designs?

Take the null hypothesis to be that any of the 150 considered buildings could be included in the top 10 with equal likelihood. How likely under this model would we see 4 (or more) of Ranchello's buildings in the top 10? What do you conclude? 

Proceed by treating this as a formal hypothesis test. Define the null and alternative hypotheses, define your test statistic, produce a distribution of simulated test statistics from the null model and finish by calculating a p-value and providing your own interpretation.

> Null Hypothesis (H₀): Marcio's buildings are no more likely than any other building to be selected in the top 10 designs. This means any of the 150 buildings has an equal chance of being in the top 10.

> Alternative Hypothesis (H₁): Marcio's buildings are more likely to be selected in the top 10 designs, implying that the success rate of his buildings is higher than the others (suggesting that his designs are of higher quality).

> The hypergeometric probability mass function is given by: P(X=k)=C(30,k)*C(120,10-k)/C(150,10)

> where 
- X is the number of buildings built by the same person in the top 10.
- C(30,k) is the number of ways to choose k buildings from the 30 build by the person 
- C(120,10-k) is the number of ways to choose the remaining buildings from the 120 other buildings.
- C(150,10) is the total number of ways to choose 10 buildings from the full set of 150.
We want to calculate P(X>=4)

```{r}
# Set up parameters
total_buildings <- 150  # Total number of buildings
person_buildings <- 30  # Number of buildings by the person
top10_buildings <- 10   # Number of top 10 buildings
success_in_top10 <- 4   # At least 4 buildings by the person

# Calculate the probability of getting 4 or more of the person's buildings in the top 10
p_value <- 1 - phyper(success_in_top10 - 1, person_buildings, total_buildings - person_buildings, top10_buildings)

cat("The p-value is:", p_value, "\n")
```

> The p-value suggests the ranking could be a result of chance, implying that submitting more designs increases the likelihood of some of them making it into the top 10.
We do not have enough evidence to reject H0.

```{r}
# Set up parameters
#total_buildings <- 150      # Total number of buildings
#person_buildings <- 30      # Buildings by the person
#top10_buildings <- 10       # Number of buildings selected for top 10
success_threshold <- 4      # We want to see at least 4 buildings from the person
num_simulations <- 10000   # Number of simulations

# Monte Carlo simulation
set.seed(123)  # For reproducibility
successes <- replicate(num_simulations, {
  # Randomly select 10 buildings from the 150
  selected_buildings <- sample(c(rep(1, person_buildings), rep(0, total_buildings - person_buildings)), top10_buildings)
  # Count how many of the selected buildings belong to the person
  sum(selected_buildings)
})
#successes
# Estimate the probability of getting 4 or more of the person's buildings in the top 10
p_value <- mean(successes >= success_threshold)
# Print result
cat("Estimated probability (p-value) of getting 4 or more of the person's buildings in the top 10:", p_value, "\n")
```

> The simulation above is just confirming the result from the phyper() function.

## Problem 3: Permutation testing <small>(8pts)</small>

Below are data arising from a (fictionalized) data source: the number of defects per day on an assembly line before and after installation of a new torque converter (this is a totally fictional "part" of an assembly line--just treat these as "control" and "treatment" groups, respectively).

```{r}
before = c(4,5,6,3,6,3,4,5,5,3,4,6,4,6,3,4,2,2,0,7,5,8,4,5,1,4,4,8,2,3)
after  = c(3,2,4,3,7,5,5,2,2,4,5,2,2,6,1,5,6,3,2,3,7,3,4,5,4,2,2,6,7,8)
```

a) (4 points) Use a permutation test to assess the claim that installation of the new part *changed* the prevalence of defects. That is, test the null hypothesis that the distribution of defects is the same before and after installation of the new part. Produce a p-value and interpret the results of your test in context.

> Null Hypothesis (H₀): The distribution of defects before and after the installation of the new part is the same.

> Alternative Hypothesis (H₁): The distribution of defects has changed after the installation.


```{r}
# Observed difference in means
observed_diff <- mean(after) - mean(before)

# Permutation test
set.seed(123)  # For reproducibility
n_permutations <- 10000
combined <- c(before, after)
n_before <- length(before)

# Initialize a vector to store permuted differences
permuted_diffs <- numeric(n_permutations)

for (i in 1:n_permutations) {
  permuted <- sample(combined)  # Shuffle the combined data
  permuted_diffs[i] <- mean(permuted[1:n_before]) - mean(permuted[(n_before + 1):length(combined)])
}

# Calculate the p-value (two-sided test)
p_value <- mean(abs(permuted_diffs) >= abs(observed_diff))

# Results
cat("Observed Difference in Means:", observed_diff, "\n")
cat("P-value:", p_value, "\n")

```


b) (4 points) Explain, briefly, what you did above and why. Imagine that you are trying to explain to someone who isn't well versed in statistics what exactly you are doing in a permutation test. Explain your conclusion based on your test above. Three to five sentences should be plenty, but you are free to write as much or as little as you think is necessary to clearly explain your findings.

> In a permutation test, we assess whether the observed difference in defects before and after installing a new part is due to random chance. By randomly shuffling the data and recalculating differences, we create a distribution that reflects no real effect. The large p-value obtained suggests that the observed change in defect rates is likely due to random variation rather than a significant effect from the new part. Thus, we do not have sufficient evidence to conclude that the installation significantly altered the number of defects on the assembly line.


## Problem 4: Memes <small>(8pts)</small>

The following question comes from Karl Rohe, who developed the very first version of this class. This question has been reproduced in nearly the exact original (very amusing) wording.

> **Memes, part 1** (Please forgive me. I drank too much coffee before writing this question.)
> 
> In class thus far, there have been 416 comments posted in the bbcollaborate chat during class. An expert panel has judged 47 of these comments to be memes. The big-bad-deans say that they are concerned "if there is evidence that more than 10% of comments are memes." So, this looks like bad news, 47/416>10%.
> 
> Karl pleads with the deans: "Please, oh please, you big-bad-deans... Memeing is totally random." (I don't actually know what this notion of "random" means, but please just run with it for this question.) Then, along comes you, a trusty and dedicated 340 student. You say that "because we have only observed 416 comments, we don't really know what the 'true proportion' of memes."
> 
> 4a: What would be a good distribution for the number of memes?
> 
> 4b: Using your distribution from 4a, test the null hypothesis that the 'true proportion' is actually 10%. It's all up to you now... report the p-value.

Hints:

- For 4a, there should be a (hopefully) fairly intuitive choice of random variable that makes sense here. Look at your list of random variables and ask yourself which of these makes the most sense.

> A suitable distribution for modeling the number of memes in the comments is the binomial distribution. This choice is appropriate because we are dealing with a fixed number of independent trials—specifically, the 416 comments made during class—where each comment can either be classified as a meme (success) or not (failure). We can represent the number of memes, denoted as X, as a binomial random variable: X∼Binomial(n=416,p=0.10) , where n = 416 is the total number of comments and p=0.10 is the hypothesized true proportion of memes according to the null hypothesis. This framework allows us to assess the likelihood of observing the given number of memes under the assumption that the true proportion is indeed 10%.


- For 4b, you can use the built-in function in R to simulate observations according to your null. Remember that you **always simulate *assuming* the null hypothesis**. Make sure your choice of the necessary parameter(s) reflects this assumption.

```{r}
# Parameters
n <- 416           # Total number of comments
p_null <- 0.10    # Null hypothesis proportion

# Observed number of memes
observed_memes <- 47

# Simulate the number of memes under the null hypothesis
set.seed(123)  # For reproducibility
n_simulations <- 10000
simulated_memes <- rbinom(n_simulations, n, p_null)

# Calculate the p-value (one-tailed test)
p_value <- mean(simulated_memes >= observed_memes)

# Results
cat("P-value:", p_value, "\n")

```

> In conclusion, the binomial distribution provides a robust framework for modeling the number of memes in the comments, given the fixed number of independent trials and the binary outcome for each comment. By using the parameters n=416 and p=0.10, we can assess the likelihood of observing the current number of memes. This approach allows us to rigorously test the null hypothesis regarding the true proportion of memes, offering valuable insights into whether the proportion exceeds the 10% threshold suggested by the deans.

> We conclude that there is no enough evidence to reject H0. So among comments 10% are memes.
