---
title: "Monte Carlo Testing: Additional Examples"
output: 
  html_document:
    self_contained: yes
---

```{r,echo=F}
knitr::opts_chunk$set(cache=T)
```

## Example: Groundhog's Day

How well has Punxsutawney Phil done in predicting the weather?

Thousands gather at Gobbler’s Knob in Punxsutawney, Pennsylvania, on the second day of February to await the spring forecast from a groundhog known as Punxsutawney Phil. According to legend, if Phil sees his shadow the United States is in store for six more weeks of winter weather. But, if Phil doesn’t see his shadow, the country should expect warmer temperatures and the arrival of an early spring.

Source: https://www.kaggle.com/datasets/groundhogclub/groundhog-day?resource=download

```{r}
groundhog<-read.csv("archive.csv")

#Get rid of rows with no record or partial shadow. Let's be serious!

groundhog <- subset(groundhog, groundhog$Punxsutawney.Phil %in% c("Full Shadow","No Shadow") & !is.na(groundhog$February.Average.Temperature))

```

Let's do a permutation test

$H_0:$ Phil's not a true forecasting groundhog
$H_1:$ Phil has some forecasting power.

We will compare mean temperature in February as a measure of early spring. We will take those years of "early spring" prediction and those of "regular spring" and compare the average average Feb temperature in those two groups.

```{r}
#Isolate feb avg temperature for full shadow years
#isolate feb avg temperature for no shadow years
feb.avg.shadow <- groundhog$February.Average.Temperature[groundhog$Punxsutawney.Phil=="Full Shadow"]
feb.avg.noshadow <- groundhog$February.Average.Temperature[groundhog$Punxsutawney.Phil=="No Shadow"]

#We will use mean feb no shadow - mean feb shadow; if this >0 that is evidence that the predictions work.

mean(feb.avg.noshadow)-mean(feb.avg.shadow)

permute_and_compute <- function(sampleA, sampleB){
  #remember - sampleA is no shadow
  #sample B is shadow
  pooledData <- c(sampleA, sampleB)
  shuffledData <- sample(pooledData)
  sim.sampleA <- shuffledData[1:length(sampleA)]
  sim.sampleB <- shuffledData[(length(sampleA)+1):length(shuffledData)]
  #we may modify this if we want to use a different test statistic
  return(mean(sim.sampleA)-mean(sim.sampleB)) 
}
t_obs <- mean(feb.avg.noshadow)-mean(feb.avg.shadow)

test.stats <- 0 #lazy empty vector - R will add more to it without complaining
NMC <- 10000
for(i in 1:NMC){
  test.stats[i] <- permute_and_compute(feb.avg.noshadow, feb.avg.shadow)
}
hist(test.stats)
abline(v=t_obs, col="red")

mean(test.stats >= t_obs)

```

Maybe he's better at predicting march temperatures?
```{r}
mar.avg.shadow <- groundhog$March.Average.Temperature[groundhog$Punxsutawney.Phil=="Full Shadow"]
mar.avg.noshadow <- groundhog$March.Average.Temperature[groundhog$Punxsutawney.Phil=="No Shadow"]

t_obs <- mean(mar.avg.noshadow)-mean(mar.avg.shadow)

test.stats <- 0 #lazy empty vector - R will add more to it without complaining
NMC <- 10000
for(i in 1:NMC){
  test.stats[i] <- permute_and_compute(mar.avg.noshadow, mar.avg.shadow)
}
hist(test.stats)
abline(v=t_obs, col="red")

mean(test.stats >= t_obs)
```

What if we averge Feb and March temperatures together.
```{r}
groundhog$FebMarAvg <- (groundhog$February.Average.Temperature+groundhog$March.Average.Temperature)/2
avg.shadow <- groundhog$FebMarAvg[groundhog$Punxsutawney.Phil=="Full Shadow"]
avg.noshadow <- groundhog$FebMarAvg[groundhog$Punxsutawney.Phil=="No Shadow"]

t_obs <- mean(avg.noshadow)-mean(avg.shadow)

test.stats <- 0 #lazy empty vector - R will add more to it without complaining
NMC <- 10000
for(i in 1:NMC){
  test.stats[i] <- permute_and_compute(avg.noshadow, avg.shadow)
}
hist(test.stats, breaks=50)
abline(v=t_obs, col="red")

mean(test.stats >= t_obs)

```
I don't know about you, but this is staring to seem a little bit weird. If we did a permutation test, 


What do we consider an early spring? 
If both Feburary temperatures and march temperature are greater than average, then we say we have an early spring.
```{r}
feb.avg <- mean(groundhog$February.Average.Temperature)
mar.avg <- mean(groundhog$March.Average.Temperature)

groundhog$EarlySpring <- groundhog$February.Average.Temperature>feb.avg & groundhog$March.Average.Temperature>mar.avg

addmargins(table(groundhog$Punxsutawney.Phil, groundhog$EarlySpring))
```

when earlySpring = TRUE, and prediction was "no shadow" that would be correct
when earlySpring = FALSE and prediction is "full shadow" that would be correct

```{r}
accuracy <- function(guesses, weather){
  predictEarlySpring <- guesses=="No Shadow"
  nCorrect <- sum(weather==predictEarlySpring)
  return(nCorrect/length(weather))
}

accuracy(groundhog$Punxsutawney.Phil, groundhog$EarlySpring)
```

Let's call our simulated Groundhog "Bernoulli Phil"

```{r}
prop.table(table(groundhog$Punxsutawney.Phil))
#A randomly guessing groundhog would see his shadow this proportion of the time

p.shadow <- prop.table(table(groundhog$Punxsutawney.Phil))[1]
```


```{r}
(accuracy.obs <- accuracy(groundhog$Punxsutawney.Phil, groundhog$EarlySpring))

#simulate guesses by Bernoulli Phil
randomGuesses <- function(n=115, p=0.8695652){
  return(sample(c("Full Shadow","No Shadow"), size=n, replace=TRUE, prob=c(p,1-p)))
}

NMC <- 10000
results <- rep(0,NMC)
n <- nrow(groundhog)

for(i in 1:NMC){
  results[i] <- accuracy(randomGuesses(n, p.shadow), groundhog$EarlySpring)
}

hist(results)
abline(v=accuracy.obs)

mean(results >= accuracy.obs)

```



The scatterplot colored by shadow status makes this pattern suspicously clear

```{r}
plot(groundhog$February.Average.Temperature, groundhog$March.Average.Temperature, col=(as.numeric(groundhog$Punxsutawney.Phil=="Full Shadow") +1), xlab="Avg. Feb Temp", ylab="Avg. Mar Temp", main="National Feb/Mar Temp vs Shadow")
abline(v=mean(groundhog$February.Average.Temperature))
abline(h=mean(groundhog$March.Average.Temperature))
legend(x=25, y=50, legend=c("No Shadow","Full Shadow"), pch=1, col=1:2)
```

O ye unbelievers! Witness the prognosticating powers of Phil the groundhog!


However we have to be cautious - think about climate change and Phil's predictions.
```{r}
plot(groundhog$February.Average.Temperature, col=as.factor(groundhog$Punxsutawney.Phil), pch=16)
```
Notice that he has been predicting early springs more often since the 60s, and also we've been having warmer and warmer temperatures. Perhaps he's predicting early spring more often because of climate change, and thus he is correct more often. 

This whole analysis really hinges on what we mean by an "early spring". One could argue that with rising global temperatures, spring is actually coming earlier more often than it used to, and THAT's why he's seeing no shadow more often. 

If we repeat this analysis and look not at whether the temperature is above average, but rather whether the temperature is above the `moving average` then we get totally different conclusions.



## Example: Comparing Consistency

Student A and Student B are in the same class. We are curious to know about their academic performance, in particular their consistency in grade performance. 

```{r}
A.grades <- c(83.4,91.4,83.0,77.2,81.9,76.8,82.4,89.0,82.0,74.9,77.6,78.7,79.1,77.8,80.3)
B.grades <- c(75.9,66.4,98.1,83.0,72.4,70.6,67.1,73.0,89.1,87.0,83.9,63.7,88.6,90.0,75.5,86.5,77.5)
A.grades <- A.grades - mean(A.grades)
B.grades <- B.grades - mean(B.grades)
boxplot(A.grades, B.grades, horizontal=TRUE)

```

The question is this: are the two students equally consistent in their grades? Let's think about consistency in terms of variance

Let's fill in the following code:
```{r}

#Takes in two data vectors, compares them in a meaningful way and returns a statistic that is useful to compare consistency.
testStat <- function(dataA, dataB){
  return(var(dataA)-var(dataB))
}

#This function takes in two vector, combines them, shuffles,
#And splits them
#Then calculates a test statistic
permuteAndCompute <- function(dataA, dataB){
  combinedData <- c(dataA, dataB)
  shuffledData<- sample(combinedData)
  simA <- shuffledData[1:length(dataA)]
  simB <- shuffledData[-(1:length(dataA))] #I use negative index
  return(testStat(simA, simB))
}

#Takes in the two data sets and a number of monte carlo runs. It will perform the MC simulation NMC times and it will return a distribution of test statistics.
mcDist <- function(dataA, dataB, MNC){
  results <- 0
  for(i in 1:NMC){
    results[i] <- permuteAndCompute(dataA, dataB)
  }
  return(results)
}
```

```{r}
obsTestStat <- testStat(A.grades, B.grades)
mcSamplingDistribution <- mcDist(A.grades, B.grades, 10000)

hist(mcSamplingDistribution)
abline(v=obsTestStat)
obsTestStat
mean(mcSamplingDistribution >= obsTestStat)
mean(mcSamplingDistribution <= obsTestStat)

```
We have about .1% in the left-tail, more extreme than our observed test statitic. However the question we were investigating is "Are they equally consistent or not"

So - if the test statisitc was very positive (+75 or so) we would also consider that evidence for the alternative.

This is a two-sided test; we need to calculate probability from both sides of the distribution. What we want to do then is to follow this model:

1. calculate the probability <= obs. test stat
2. calculate the probability >= obs. test stat
3. take the smaller of the two
4. Double it

```{r}
#This is my formula for a 2-tailed p-value
2*min(mean(mcSamplingDistribution>=obsTestStat),
      mean(mcSamplingDistribution<=obsTestStat))
```
If this was a symmetric distribution, this would be the same as:
```{r}
mean(mcSamplingDistribution>= abs(obsTestStat)) + mean(mcSamplingDistribution<= -abs(obsTestStat))

```


Let's suppose we used some other measure of "difference of consistency"
For example, we could take the ratio of the variances.
```{r}

#redefine the test statistic
testStat <- function(dataA, dataB){
  return(var(dataA)/var(dataB))
}

obsTestStat <- testStat(A.grades, B.grades)
mcSamplingDistribution <- mcDist(A.grades, B.grades, 10000)

hist(mcSamplingDistribution, breaks=50)
abline(v=obsTestStat)
2*min(mean(mcSamplingDistribution >= obsTestStat),
      mean(mcSamplingDistribution <= obsTestStat))


```



## Test statistics and where to find them

Crucial to our discussion above was that we have our function that measured how "unusual" or "extreme" our data is.
This function is called a *test statistic* because it is a function of the data (hence a "statistic") and because we use it in the test of our hypothesis.

Sometimes it's really obvious what our test statistic should be-- like counting how many cups our tea taster got right.
Sometimes it's less obvious.
We'll see examples like that later in the semester.

Let's look at another example, which will also serve to illustrate why the "right" test statistic isn't always obvious.

## Example: flipping a (possibly biased) coin

Suppose we have a coin, and we are interested in whether or not the coin is "fair".
Here, by "fair", we mean that when flipping the coin, it is equally likely to land heads or tails.
Let's use $p \in [0,1]$ to denote the (unknown) probability that the coin lands heads.
Our null hypothesis might be that the coin is indeed fair, which we would write as
$$
H_0 : p = \frac{1}{2}.
$$

We read that as something like "H nought is that $p$ is 1/2" or "the null hypothesis is that $p=1/2$".

Here is the result of flipping this coin 200 times:
```{r}
flips <- "HTTHTHHTTHTTTHTHHTTHHTHHTHHHHHTTTTHHHHHHHHTHHTHHHTTTHTTTHHHTHHHHHHTHTTHTTHTHTTHHHHTHHHTHHTTHTTTTTTHHHTTHTHTHHTTHHHTTTHHTHHHTTHHTTTTTHTHHTTTHTTHTTHHTTTTTHTTTHTHHHHTTTTTHTTTHHHHHTHHTHTHHTTTTTTHHHTTTHTTT"
```

How might we go about testing our null hypothesis that $p=1/2$?
Well, we have to start by asking what we expect the data to "look like" if the probability of heads is actually $1/2$, and then come up with a test statistic that captures that.

__Question:__ let's pause and discuss what some possible choices for our test statistic would be.

___

___

___

Okay, there are of course lots of possibilities to choose from, some better than others.

Certainly a sensible choice, though, would be to count how many of the 200 coinflips landed heads.
If the coin is truly fair, i.e., if $p=1/2$,  then we expect about half of the coinflips to land heads.
Let's implement that.

```{r}
count_heads <- function( coinflips ) {
  # We're implicitly assuming coinflips is a vector of one-character strings.
  # Just a reminder that in non-demo code, we usually want to include some
  # error checking to make sure that our function's arguments are
  # as we expect.
  return( sum( coinflips==rep('H', length(coinflips)) ) );
}
```

Okay, let's apply this statistic to our `flips` data.

```{r}
# We need to turn our string of coin flips into a vector before
# we pass it into `count_heads()`
# strsplit( x, split=s) splits the string x on the separating character s,
# and returns a list structure. See ?strsplit for more.
# unlist( v ) turns a list structure into a flat vector that is easier
# to work with. See ?unlist for details.
count_heads( unlist( strsplit( flips, split='' ) ) );
```

Okay, now, we've chosen our test statistic, which measures how "unusual" or "extreme" our data is.
But we don't know how unusual is unusual, or how extreme is extreme.
That is, we don't know what this number *would* look like, if the null hypothesis were true.

Let's simulate some data under the null hypothesis to get an idea.

```{r}
simulate_coinflips <- function(n, p) {
  flips <- sample( c('H','T'), size=n, replace=TRUE, prob=c(p, 1-p) );
  # flips is now a vector of 'H' and 'T's,
  # which is what count_heads expects, so let's return it.
  # If we were going to print this or something, I might prefer to
  # return a string using paste( flips, sep='')
  return( flips );
}

# Now let's simulate a bunch of coinflips
NMC <- 500;
# We're going to make a histogram of the number of heads that show up
# in each experiment run, so create a vector to store those in.
counts <- rep( 0, NMC );
for( i in 1:NMC ) {
  counts[i] <- count_heads( simulate_coinflips( 200, 0.5 ) );
  cat(counts[i])
}

# Now, make a histogram. As usual, we could use ggplot2 to make things
# look a bit nicer, but we just want something quick and simple, here.
hist( counts ) 
```

Okay, so that vast majority of the time, the number of heads in 200 fair coin flips is between 80 and 120.
Once in a while, of course, it's more than that or less than that.

Now, our coin in question could be unfair in two different ways-- either $p$ is smaller than $1/2$ or $p$ is larger than $1/2$.

If $p$ is smaller than $1/2$, then we expect there to be a lot fewer than $200/2 = 100$ heads.
If $p$ is larger than $1/2$, then we expect there to be a lot more than $100$ heads.

So an "unusual" result would presumably correspond to a number of heads being much larger *or* much smaller than $100$.
This corresponds to what is called a *two-sided test*-- we are concerned about the true value of $p$ being *either* higher or lower than our null value $p = 1/2$.

This is in contrast to a one-sided test, in which we might, for example, only be concerned with $p$ being smaller than $1/2$.
We'll have lots more to say about this next week when we start talking more about testing and p-values.

For now, let's just note that our data observed in `flips`, has a number of coin flips well within our "usual" range of 80 to 120.
```{r}
count_heads( unlist( strsplit(flips, split='')) )
```

Now, here's a different sequence of coin flips:

```{r}
more_flips <- 'HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT'
```

This sequence of flips has 100 heads and 100 tails:

```{r}
count_heads( unlist( strsplit( more_flips, split='')))
```

According to our test statistic, this is not an unusual outcome at all-- in 200 flips of a fair coin, we expect there to be around 100 heads and 100 tails.
Yet I think we can all agree that there's something unusual about the sequence of coin flips above.

If you've seen the play *Rosencrantz and Guildenstern Are Dead* (or watched the movie based on the play), this might ring a bell.
For those not familiar, see [this clip](https://www.youtube.com/watch?v=C_TfdNAXOwE).

So here we see that our test statistic doesn't capture all of the ways that our data could be "extreme" or "unusual".
This is precisely why devising a "good" test statistic can be hard!

There are a lot of different ways that our data can be "weird", and we need to be careful that we capture the right notion of weirdness!



## Example: hot hands

For the next example, let's do something slightly more complicated.

A certain professional basketball player believes he has "[hot hands](https://en.wikipedia.org/wiki/Hot_hand)" when shooting 3-point shots (i.e. if he makes a shot, he’s more likely to also make the next shot). His friend doesn’t believe him, so they make a wager and hire you, a statistician, to settle the bet.

As a sample, you observe the next morning as the player takes the same 3-point shot 200 times in a row (assume he is well rested, in good physical shape, and doesn’t feel significantly more tired after the experiment), so his level of mental focus doesn’t change during the experiment). You obtain the following results, where Y denotes a success and N denotes a miss:

```
YNNNNYYNNNYNNNYYYNNNNNYNNNNNNNNNYNNNNNYYNYYNNNYNNNNYNNYYYYNNYYNNNNNNNNNNNNNNNYYYNNNYYYYNNNNNYNYYNNNNYNNNNNNYNNNYNNYNNNNNYNYYYNNYYYNYNNNNYNNNNNNNYYNNYYNNNNNNYNNNYNNNNNNNNYNNNYNNNNNYYNNNNNNYYYYYYNYYNNYN
```

Note that the existence of a "hot hands" effect means the shots are not independent. Also note that there's a third possibility: that the player is more likely to "[choke](https://en.wikipedia.org/wiki/Choke_(sports))" and miss the next shot if he scored the previous one (e.g. maybe scoring a shot makes him feel more nervous because he feels like he's under pressure).

### Attempt 1: run length

Since the existence of a hot hands effect tends to increase the run lengths of `Y`s compared to if the shots were independent, we can use the longest run length as a way of comparing independence vs hot hands (note if the player is a choker, they will tend to have shorter runs of `Y`s than if they were independent, so you can simply ignore this case for now and compare hot hands v. independence for simplicity).

Now, how exactly do you compare these two situations and determine which is a better fit for the data?

One thing that's worth noting is that ***if a sequence of repeated experiments is independent, then it shouldn't matter what order the results are in***. This should be fairly easy to understand and agree with.

Let's ***assume that the throws are totally independent***. Recall we also assume he doesn't get tired so his baseline shot-making ability doesn't change over the course of the experiment. Therefore, we should be able to (under these assumptions) ***arbitrarily reorder his shots without affecting any statistical properties of his shot sequence***. So let's do that!

We begin by parsing the throws into a vector of `Y` and `N`.

```{r}
# the sequence of throws is broken up into 4 chunks for readbility, then
# paste0 is used to merge them into a single sequence, then
# strplit("YN...N",split="") is used to split the string at every "", so
# we get a vector of each character, and finally
# [[1]] is used to get the vector itself (strsplit actually outputs a list
# with the vector as the first element; [[1]] removes the list wrapper)
# 
# for more info about the strsplit function, see
# https://www.journaldev.com/43001/strsplit-function-in-r

throws = strsplit(
   paste0("YNNNNYYNNNYNNNYYYNNNNNYNNNNNNNNNYNNNNNYYNYYNNNYNNN",
          "NYNNYYYYNNYYNNNNNNNNNNNNNNNYYYNNNYYYYNNNNNYNYYNNNN",
          "YNNNNNNYNNNYNNYNNNNNYNYYYNNYYYNYNNNNYNNNNNNNYYNNYY",
          "NNNNNNYNNNYNNNNNNNNYNNNYNNNNNYYNNNNNNYYYYYYNYYNNYN"), split="")[[1]]

throws
```


Next, we write a function to get the longest run of `Y`s in the throw sequence. Here we use a convenient function called `rle( )` which is short for [run length encoding](https://en.wikipedia.org/wiki/Run-length_encoding), which turns our sequence of throws into sequences of runs (e.g. YNNNNYYNNNY becomes something like "1 `Y`, 4 `N`s, 2 `Y`s, 3 `N`s, and 1 `Y`"). We can then simply take the longest of the `Y` runs.


```{r}
longestRun = function(x,target = 'Y'){
    max(0,with(rle(x), lengths[values==target]))
}

longestRun(throws)
```



Now, we randomly shuffle the sequence of throws many times and see what the longest `Y` runs look like for these shuffled sequences.

```{r}
# set number of reps to use
MCN = 10000

# create vector to save results in
mc.runs = rep(0,MCN)

# for each rep, randomize sequence and find longest run of Y
for(i in 1:MCN){
    mc.runs[i] = longestRun(sample(throws))
}
```


```{r}
options(max.print=500)
mc.runs
```


```{r}
barplot(table(mc.runs))
```
```{r}
mean(mc.runs >= 6)
```

compared to other shuffled sequences, our run length doesn't seem that unlikely. Therefore, this method seems inconclusive.

Can we find an even better "statistic" to use?

### Attempt 2: running odds ratio

Consider **every pair of consecutive throws** and make a table of the outcomes. For example, the first 8 throws in the sequence are YNNNNYYN. Breaking this into consecutive pairs, we have YN, NN, NN, NN, NY, YY, YN. This gives the table:

<center>
<div style="width:100px;">

| NN | NY | YN | YY |
|:--:|:--:|:--:|:--:|
| 3  | 1  | 2  | 1  |

</div>
</center>

Suppose we do this for the entire sequence of 200 throws (note this gives you 199 pairs). If we **divide the number of NY by the number of NN**, we get an estimate for **how much _more_ likely he is to make the next shot _assuming he missed his last shot_**.

Similarly, we can **divide the number of YY by the number of YN** to get an estimate for **how much _more_ likely he is to make the next shot _assuming he scored his last shot_**.

Now, note that **if the "hot hands" effect really exists** in the data, then **YY/YN should be larger than NY/NN** in a large enough sample. We use this fact to define the following quantity:

$$R=\frac{(\text{# of YY})/(\text{# of YN})}{(\text{# of NY})/(\text{# of NN})}$$

The ratio $R$ represents, in some sense, **how much more likely** the player is to **make the next shot** if he **made the previous shot _vs_ if he didn't make the previous shot** (note the **_vs_**). This is exactly what we're trying to investigate!

If there is a "hot hands" effect, the numerator should be greater than the denominator and we should have $R>1$. If the throws are independent and do not affect each other then in theory we should have $R=1$. If the player is actually a choker (i.e. he is more likely to miss after a successful shot), then we should have $R<1$. (Side note: this is basically an [odds ratio](https://journalfeed.org/article-a-day/2018/idiots-guide-to-odds-ratios)).

Now, we can use the same general method as the first attempt. If we assume his throws are independent and his shot probability doesn't change significantly during the experiment, then we can randomly shuffle his throws and no properties should change. So let's do that!

First, I wrote a function to split the sequence of throws into consecutive pairs and then tabulates them.


```{r}
# define function for tabulating consecutive pairs
tableOfPairs = function(x) {n=length(x);Rfast::Table(paste(x[1:(n-1)],x[2:n],sep=""))}

# test function for correct output
tableOfPairs(strsplit("YNNNNYYN",split="")[[1]])
```



```{r}
# run function on original sequence of throws
tableOfPairs(throws)
```

Next, I wrote a function that takes the above table as an input and returns the ratio R as defined above.


```{r}
ratioFromTable = function(tb) setNames((tb["YY"]/tb["YN"])/(tb["NY"]/tb["NN"]),"R")

# run on our data
ratioFromTable(tableOfPairs(throws))
```


```{r}
# we can check this is correct by manually computing it
(28/35)/(34/102)
```


Now we just need to shuffle the sequence and see what this ratio looks like for other sequences.


```{r}
# set number of reps to use
N = 100000

# create another vector to save results in
mc.ratios = rep(NA,N)

# for each rep, randomize sequence and find ratio R
for(i in 1:N){
    mc.ratios[i] = ratioFromTable(tableOfPairs(sample(throws)))
}

# alternatively, use replicate
mc.ratios = replicate(1e5,ratioFromTable(tableOfPairs(sample(throws))))
```


```{r}
options(max.print=500)
round(mc.ratios,2)
```


```{r}
hist(mc.ratios)
```

```{r}
mean(mc.ratios>=ratioFromTable(tableOfPairs(throws)))
```

Now we can see our original ratio of $R=2.4$ seems extremely unlikely! In particular, most of the shuffled statistics are centered around 1 (which is what we expect, since we established $R=1$ for independent sequences).

This method (which is a little more refined than the simpler run length method) appears to show that our original sequence isn't well explained by the throws being independent. Since $R=2.4\gg1$ and this result appears unlikely to happen under independence, we may conclude **the player does actually have hot hands**.


## Random Visual Noise detection

```{r}
obsData <- matrix(
           c(0,0,0,1,1,1,0,1,1,1,0,1,1,0,0,1,
             1,1,0,1,0,1,0,0,0,0,1,1,0,1,0,1,
             0,0,1,1,0,1,0,1,1,0,1,1,0,1,0,1,
             0,0,1,0,1,1,0,1,1,1,0,0,0,1,0,1,
             1,1,1,1,0,1,1,1,0,1,1,1,1,0,0,1,
             1,0,0,0,0,1,1,1,1,1,0,0,1,1,1,0,
             1,0,1,0,1,0,0,0,1,1,0,0,0,1,0,1,
             0,1,1,0,1,0,0,0,1,1,1,1,0,1,1,1,
             0,0,1,1,1,1,1,0,0,0,1,0,1,1,1,0,
             0,0,1,1,0,0,0,1,0,1,1,0,1,1,1,1,
             0,1,1,1,1,1,1,0,1,1,1,1,1,0,1,1,
             1,0,0,1,1,0,0,1,0,0,1,0,0,1,0,0,
             0,1,0,1,1,1,1,1,0,1,1,0,1,1,0,0,
             1,0,1,0,1,0,0,0,0,1,0,1,1,1,1,1,
             1,0,0,0,0,1,0,0,1,0,0,0,0,1,1,0,
             0,1,1,1,0,1,1,0,1,1,0,1,0,1,0,1),
           nrow=16,byrow=TRUE)
image(obsData, useRaster=TRUE, axes=FALSE)
```
Let's get some summary stats

IDea: Horizontal Symmetry

```{r}
calculate_horizontal_symmetry <- function(matrix){
  nCols <- dim(matrix)[2]
  nSame = 0
  for(i in 1:(nCols/2)){
    nSame <- nSame + sum(matrix[,i]== matrix[, nCols+1-i])
  }
  return(nSame)
}

NMC <- 10000
results <- 0
for(i in 1:NMC){
  shuffledMatrix <- matrix(data=sample(obsData), nrow=dim(obsData)[1], byrow=TRUE)
  results[i] <- calculate_horizontal_symmetry(shuffledMatrix)
}
hist(results)
calculate_horizontal_symmetry(obsData)
```





Look at vertical neighbors

A 00 edge will sum to 0
A 01 or 10 edge will sum to 1
A 11 edge will sum to 2
```{r}
vEdges <- obsData[-1,]+obsData[-16,]
hEdges <- obsData[,-1]+obsData[,-16]
table(vEdges)
table(hEdges)
```
```{r}
#Add two tables together!
table(vEdges)+table(hEdges)
```

```{r}
#Let's let the BW edges be the test statistic to see how well it works:
count_bw_edges <- function(matrix){
  dims <- dim(matrix)
  vEdges <- matrix[-1,]+matrix[-dim(matrix)[1],]
  hEdges <- matrix[,-1]+matrix[,-dim(matrix)[2]]
  bwEdges <- as.numeric((table(factor(vEdges, levels=0:2))+
                           table(factor(hEdges, levels=0:2)))[2])
  return(bwEdges)
}

count_bw_edges(obsData)
```

```{r}
NMC <- 5000
results <- 0
for(i in 1:NMC){
  shuffledMatrix <- matrix(data=sample(obsData), nrow=dim(obsData)[1], byrow=TRUE)
  results[i] <- count_bw_edges(shuffledMatrix)
}
hist(results)
```


Let's see how well this test statistic does against some image that is definitely not random:

```{r}
obsData2 <- matrix(rep(c(0,1),8*16),nrow=16, byrow=TRUE)
image(obsData2, useRaster=TRUE, axes=FALSE)

count_bw_edges(obsData2)
```

```{r}
obsData3 <- matrix(rep(c(rep(c(0,0,1,1),8),rep(c(1,1,0,0),8)),4), ncol=16, byrow=TRUE)
image(obsData3, useRaster=TRUE, axes=FALSE)
count_bw_edges(obsData3)

```



Let's just create a function to automate the MC simulation
```{r}
generateMCsim <- function(data, NMC){
  results <- 0
  for(i in 1:NMC){
    shuffledMatrix <- matrix(data=sample(data), nrow=dim(data)[1], byrow=TRUE)
    results[i] <- count_bw_edges(shuffledMatrix)
  }
  return(results)  
}
```
And now the Monte Carlo part.
```{r}
mcDist <- generateMCsim(obsData2,5000)
hist(mcDist)
count_bw_edges(obsData2)

```


Moran's I
We define neighbors to be cardinally adjacent

$$I = \frac{N}{W} \frac{\sum_{i=1}^N \sum_{j=1}^N w_{ij}(x_i-\bar{x})(x_j-\bar{x})}{\sum_{i=1}^N(x_i-\bar{x}^2)}$$


```{r}
moranI <- function(matrix){
  N <- prod(dim(matrix))
  #The number of adjacencies is the product of the dimensions -1
  W <- prod(dim(matrix)-1)
  xbar <- mean(matrix)
  nRow <- dim(matrix)[1]
  nCol <- dim(matrix)[2]
  I <- 2*(sum((matrix-xbar)[-1,]*(matrix-xbar)[-nRow,]) +
    sum((matrix-xbar)[,-1]*(matrix-xbar)[,-nCol]) )/
    sum((matrix-xbar)^2) * N/W
  return(I)
}
```



Or let's count the number of contiguous clumps

```{r, warning=F, message=F}
library(igraph)
library(raster)

nClumps <- function(matrix){
   return(max(as.matrix(clump(raster(matrix), directions=4)), na.rm=TRUE))
}

nClumps(obsData) + nClumps(1-obsData)
```

Distribution of clump counts
```{r}
NMC <- 1000
results <- 0
  for(i in 1:NMC){
    shuffledMatrix <- matrix(data=sample(obsData2), nrow=dim(obsData2)[1], byrow=TRUE)
    results[i] <- nClumps(shuffledMatrix) + nClumps(1-shuffledMatrix)
  }
hist(results)
nClumps(obsData2)+nClumps(1-obsData2)

```



How does counting clumps work on the first dataset?

```{r}
NMC <- 1000
results <- 0
  for(i in 1:NMC){
    shuffledMatrix <- matrix(data=sample(obsData), nrow=dim(obsData)[1], byrow=TRUE)
    results[i] <- nClumps(shuffledMatrix)+nClumps(1-shuffledMatrix)
  }
hist(results)
nClumps(obsData)+nClumps(1-obsData)

```



Perhaps the standard deviation of the clump sizes might be useful.
```{r}
sdClumpSize <- function(matrix){
  clumps1 <- as.vector(table(as.matrix(clump(raster(matrix), directions=4))))
  clumps0 <- as.vector(table(as.matrix(clump(raster(1-matrix), directions=4))))
  return(sd(c(clumps0,clumps1)))
}
```

```{r}
NMC <- 1000
results <- 0
  for(i in 1:NMC){
    shuffledMatrix <- matrix(data=sample(obsData), nrow=dim(obsData)[1], byrow=TRUE)
    results[i] <- sdClumpSize(shuffledMatrix)
  }
hist(results)
sdClumpSize(obsData)

```



## Coin Flips

Below are two sequences of 300 “coin flips” (H for heads, T for tails). One of these is a true sequence of 300 independent flips of a fair coin. The other was generated by a person typing out H’s and T’s and trying to seem random. Which sequence is truly composed of coin flips?

Sequence 1:

TTHHTHTTHTTTHTTTHTTTHTTHTHHTHHTHTHHTTTHHTHTHTTHTHH TTHTHHTHTTTHHTTHHTTHHHTHHTHTTHTHTTHHTHHHTTHTHTTTHH TTHTHTHTHTHTTHTHTHHHTTHTHTHHTHHHTHTHTTHTTHHTHTHTHT THHTTHTHTTHHHTHTHTHTTHTTHHTTHTHHTHHHTTHHTHTTHTHTHT HTHTHTHHHTHTHTHTHHTHHTHTHTTHTTTHHTHTTTHTHHTHHHHTTT HHTHTHTHTHHHTTHHTHTTTHTHHTHTHTHHTHTTHTTHTHHTHTHTTT

Sequence 2:

HTHHHTHTTHHTTTTTTTTHHHTTTHHTTTTHHTTHHHTTHTHTTTTTTH THTTTTHHHHTHTHTTHTTTHTTHTTTTHTHHTHHHHTTTTTHHHHTHHH TTTTHTHTTHHHHTHHHHHHHHTTHHTHHTHHHHHHHTTHTHTTTHHTTT THTHHTTHTTHTHTHTTHHHHHTTHTTTHTHTHHTTTTHTTTTTHHTHTH HHHTTTTHTHHHTHHTHTHTHTHHHTHTTHHHTHHHHHHTHHHTHTTTHH HTTTHHTHTTHHTHHHTHTTHTTHTTTHHTHTHTTTTHTHTHTTHTHTHT

Useful functions:
`flips <- strsplit("HHTHTHHTHTHHHT","")`
`with(rle(flips[[1]]), lengths[values=="H"])`

```{r}
seq1 ="TTHHTHTTHTTTHTTTHTTTHTTHTHHTHHTHTHHTTTHHTHTHTTHTHHTTHTHHTHTTTHHTTHHTTHHHTHHTHTTHTHTTHHTHHHTTHTHTTTHHTTHTHTHTHTHTTHTHTHHHTTHTHTHHTHHHTHTHTTHTTHHTHTHTHT THHTTHTHTTHHHTHTHTHTTHTTHHTTHTHHTHHHTTHHTHTTHTHTHTHTHTHTHHHTHTHTHTHHTHHTHTHTTHTTTHHTHTTTHTHHTHHHHTTTHHTHTHTHTHHHTTHHTHTTTHTHHTHTHTHHTHTTHTTHTHHTHTHTTT"

seq2 = "HTHHHTHTTHHTTTTTTTTHHHTTTHHTTTTHHTTHHHTTHTHTTTTTTHTHTTTTHHHHTHTHTTHTTTHTTHTTTTHTHHTHHHHTTTTTHHHHTHHHTTTTHTHTTHHHHTHHHHHHHHTTHHTHHTHHHHHHHTTHTHTTTHHTTTTHTHHTTHTTHTHTHTTHHHHHTTHTTTHTHTHHTTTTHTTTTTHHTHTHHHHTTTTHTHHHTHHTHTHTHTHHHTHTTHHHTHHHHHHTHHHTHTTTHH HTTTHHTHTTHHTHHHTHTTHTTHTTTHHTHTHTTTTHTHTHTTHTHTHT"

flips1 <- strsplit(seq1,"")
H.run.lengths1 <- with(rle(flips1[[1]]), lengths[values=="H"])
table(H.run.lengths1)

flips2 <- strsplit(seq2,"")
H.run.lengths2 <- with(rle(flips2[[1]]), lengths[values=="H"])
table(H.run.lengths2)
```

What if we use maximum run length as a test statistic?

```{r}
t.dist <- 0 #empty vector for our simulated distribution of test statistics
MCN <- 10000
n <- 300
for(i in 1:MCN){
  flips <- sample(c("H","T"), 300, replace=TRUE)
  H.run.lengths <- with(rle(flips), lengths[values=="H"])
  t.dist[i] <- max(H.run.lengths)
}

hist(t.dist)
```

What are the two test statistics?
We would use a 2-tailed p-value. We would consider a sequence of Hs and Ts to be weird if the max run length was too long or too short. 
```{r}
(t1 <- max(H.run.lengths1))
2*(min(mean(t.dist <= t1),mean(t.dist >= t1)))

(t2 <- max(H.run.lengths2))
2*(min(mean(t.dist <= t2),mean(t.dist >= t2)))


```
It probably comes as no surprise that the first sequence of Hs and Ts is suspicious. It does not contain enough long runs of Hs- when flips are done randomly you tend to get at least some much longer runs.

We could repeat this with the "average" run length as well.

```{r}
t.dist <- 0 #empty vector for our simulated distribution of test statistics
MCN <- 10000
n <- 300
for(i in 1:MCN){
  flips <- sample(c("H","T"), 300, replace=TRUE)
  H.run.lengths <- with(rle(flips), lengths[values=="H"])
  t.dist[i] <- mean(H.run.lengths)
}

hist(t.dist)

(t1 <- mean(H.run.lengths1))
2*(min(mean(t.dist <= t1),mean(t.dist >= t1)))

(t2 <- mean(H.run.lengths2))
2*(min(mean(t.dist <= t2),mean(t.dist >= t2)))


```
The average run length of 1.45 would virtually never happen if a coin was actually flipped 300 times. That's conclusive evidence that the first sequence was made up by a human.

What are some other test statistics that could have worked?


## Permutation tests

Apologies- this part of the document is a little messy, I have to remove duplicate work!

The results from a medical trial; larger numbers are "better"
We want to know if there is evidence that the drug (treatment group) performs on avergage better than control. 

```{r}
control <- c(-0.10, -0.55, 1.24, -0.97, -0.76,  0.21,-0.27, -1.02, 0.58, 1.67, -1.07, 0.17, 1.45, 0.34, 1.15, 0.18, -0.97, 0.43, -1.39, -2.76 );

treatment <- c( 0.54, 0.36, 0.59, -0.57, 0.53, -0.78, -0.44, -0.98, 1.31, 0.50, 0.57, 0.49, -0.96, 2.41, 0.85, 1.93, 0.95, 1.45, 1.61, -1.16 );

boxplot(treatment, control)
```

Think about the logic of how a monte carlo test would work.
We need to come up with a null model corresponding with the null hypothesis - but we really should be clear about the hypothesis that we're testing.

Claim (alt hypothesis) - the drug works. 
Null hypothesis - the drug does not work.

The null model implies - these 40 values just happened to be partitioned randomly into control group and treatment group.

So in a Monte Carlo simulation, we can take these 40 values, shuffle them around, and re-partition them into "control" and "Treatment"

The second important decision we have to make: on each simulation of the data we need to calculate some statistic or number - some function of the data - that captures the... unusualness of it somehow. Something that would lend evidence towards the alt. hypothesis. 

The alt hypothesis ("the drug works") - evidence in support of this would be: the mean for the treatment group > the mean for the control group. 

One idea: Our test statistic could be "TRUE/FALSE is the mean_Trt > mean_Cntrl"

```{r}
#Create a test_statistic function
testStat <- function(dataA, dataB){

  #true if mean dataA > mean dataB
  #return(mean(dataA) > mean(dataB)) 
  #Oops that doesn't work well, because it's only TRUE or FALSE
  
  return(mean(dataA)-mean(dataB))
}
```

Now let's get the test statistic for the observed data:
```{r}
testStat(treatment, control)

#MC simulation
NMC <- 10000
results <- FALSE
for(i in 1:NMC){
  #We need to re-partition the data into two groups.
  combinedData <- c(treatment, control)
  #we next get 'simulated' treatment and control data
  shuffledData<- sample(combinedData)
  treatmentSim <- shuffledData[1:length(treatment)]
  controlSim <- shuffledData[-(1:length(treatment))]
  results[i] <- testStat(treatmentSim, controlSim)
}
hist(results)

#Recall the alternative hypothesis:
# The drug works
# We only would consider evidence of this if the meanT-meanC >0
# So we only are going to calculate a p-value from the upper-tail
# of the distribution.

mean(results >= testStat(treatment, control))
```

It might be interesting to check what a parametric T test would give us.
```{r}
t.test(treatment, control, var.equal=TRUE, alternative="greater")
```


Some other test statistics we could possibly use:

* difference of the medians
* difference of the max values

```{r}
#Create a test_statistic function
testStat <- function(dataA, dataB){
  return(max(dataA)-max(dataB))
}

#Copied from above
NMC <- 10000
results <- FALSE
for(i in 1:NMC){
  combinedData <- c(treatment, control)
  shuffledData<- sample(combinedData)
  treatmentSim <- shuffledData[1:length(treatment)]
  controlSim <- shuffledData[-(1:length(treatment))]
  results[i] <- testStat(treatmentSim, controlSim)
}
hist(results)

mean(results >= testStat(treatment, control))

```



What if we use a different test statistic?
Let's compare the medians.
```{r}
test_statistic <- function(A,B){
  return(median(A)-median(B))
}

#rerun comparing the sample medians

(Tobsd <- test_statistic(treatment, control))
for(i in 1:NMC ) {
  test_statistics[i] <- permute_and_compute( treatment, control );
}
hist( test_statistics, breaks=60 )
abline( v=Tobsd, lw=3, col='red' )

sum(test_statistics >= Tobsd)/NMC
```


Let's use KS test statistics - the max deviance between their emprical distributions!
what is that?
```{r}
compare <- function(x, y) {
  n <- length(x); m <- length(y)
  w <- c(x, y)
  o <- order(w)
  z <- cumsum(ifelse(o <= n, m, -n))
  i <- which.max(abs(z))
  w[o[i]]
}
u<-compare(treatment, control) #this is where the max difference occurs
abs(mean(treatment < u) - mean(control < u))
```


```{r}
ecdf1 <- ecdf(treatment)
ecdf2 <- ecdf(control)
plot(ecdf1, verticals=TRUE, do.points=FALSE, col="blue", main="Compare ecdfs")
plot(ecdf2, verticals=TRUE, do.points=FALSE, add=TRUE, col='red')
lines(c(u,u), c(ecdf1(u), ecdf2(u)), lwd=2)
legend(x=min(control,treatment), y=1, legend=c("treatment","control"), col=c("blue","red"), lty=1, xjust=-1)
```

We can pull this out from the built-in `ks.test` function. The test statistic is the max deviance between the two empirical CDFs

```{r}
test_statistic <- function(A,B){
  return(as.numeric(ks.test(A,B)[1]))
}
test_statistic(treatment, control)
```

```{r}

#rerun

(Tobsd <- test_statistic(treatment, control))
test_statistics <- rep( 0, NMC ); # Vector to store our "fake" test statistics
for(i in 1:NMC ) {
  test_statistics[i] <- permute_and_compute( treatment, control );
}
hist( test_statistics )
abline( v=Tobsd, lw=3, col='red' )

sum(test_statistics >= Tobsd)/NMC
```

We can spend all our time picking different test statistics, and maybe we find one that gives us a low p-value for this data set. But we have to be careful that it would generalize well to other datasets. Furthermore, we want to make sure that it would not be giving us false positives if we were comparing two popualations that really did have the same distribution

Let's demonstrate this comparing our 3 test statistics:

* difference of means
* difference of maximums
* difference of medians
* max ecdf deviation

And we can compare them over datasets that are generated from increasingly different populations.

```{r}
generate_experiment <- function(nA, nB, effectSize){
  #we will assume that the values in our data are drawn from a distribution - 
  #here we'll use a normal distribution. We could use permutation methods but 
  #that doesn't really change the point we're trying to make.
  return(list(rnorm(nA, mean=effectSize, sd=1),
              rnorm(nB, mean=0, sd=1)))
}
#for example
generate_experiment(20,20,.5)
```
```{r}
nDatasets <- 100
NMC <- 100
effects <- seq(0,1,.1)
rr.mean <- 0; rr.max <- 0; rr.median <-0; rr.ks <- 0
for(k in 1:length(effects)){
  effect <- effects[k]
  rej.mean <- rep(FALSE,nDatasets)
  rej.max <- rep(FALSE,nDatasets)
  rej.median <- rep(FALSE,nDatasets)
  rej.ks <- rep(FALSE,nDatasets)
  #generate 100 different datasets
  for(j in 1:100){
    exp.data <- generate_experiment(20,20,effect)
    
    obs.mean <- mean(exp.data[[1]])-mean(exp.data[[2]])
    obs.median <- median(exp.data[[1]])-median(exp.data[[2]])
    obs.max <- max(exp.data[[1]])-max(exp.data[[2]])
    obs.ks <- as.numeric(ks.test(exp.data[[1]],exp.data[[2]])[1])
    
    #permute and compute 1000 times
    t.means <- 0
    t.medians <- 0
    t.maxes <- 0
    t.kss <- 0
    for(i in 1:NMC){
      shuffledData <- sample(c(exp.data[[1]],exp.data[[2]]))
      simA <- shuffledData[1:length(exp.data[[1]])]
      simB <- shuffledData[-(1:length(exp.data[[1]]))]
      
      t.means[i] <- mean(simA)-mean(simB)
      t.medians[i] <- median(simA)-median(simB)
      t.maxes[i] <- max(simA)-max(simB)
      t.kss[i] <- as.numeric(ks.test(simA,simB)[1])
    }    
    rej.mean[j] <- mean(t.means >= obs.mean) <=0.05
    rej.median[j] <- mean(t.medians >= obs.median) <=0.05
    rej.max[j] <- mean(t.maxes >= obs.max) <=0.05
    rej.ks[j] <- mean(t.kss >= obs.ks) <=0.05
  }
  rr.mean[k] <- mean(rej.mean)
  rr.max[k] <- mean(rej.max)
  rr.median[k] <- mean(rej.median)
  rr.ks[k] <- mean(rej.ks)
}

plot(x=effects, y=rr.mean, type="l", ylim=c(0,1), main="Rejection rate of different test statistics", xlab="effect size", ylab="rejection rate")
lines(x=effects, y=rr.max, col="red")
lines(x=effects, y=rr.median, col="blue")
lines(x=effects, y=rr.ks, col="darkgreen")
abline(h=.05)
legend(x=0, y=1, legend=c("means","medians","deviance","max"), col=c("black","blue","darkgreen","red"), lty=1)

```



## Random number string

Suppose I ask a computer for a string of random numbers. 

I also ask a person for a string of random numbers. 

What I want is the numbers 1 through 20 shuffled up.

Suppose I get the string, but I don't know whether it came from the computer or the person.

19, 7, 5, 4, 13, 11, 2, 12, 3, 1, 15, 10, 18, 16, 9, 17, 20, 14, 6, 8

(generated 9/26: 1 6 18 12 15 19 20 5 2 17 11 3 9 16 4 7 8 10 13 14)

How can I use a permutation test to determine the likely source of this string of numbers????

```{r}
numbers.obs <- c(19, 7, 5, 4, 13, 11, 2, 12, 3, 1, 15, 10, 18, 16, 9, 17, 20, 14, 6, 8)


#We need a way to generate a random shuffling of the numbers

generate_numbers <- function(n.numbers){
  return(sample(n.numbers))
}

```

Let's use the standard deviation of differences as a measure of randomness
```{r}
calc_sd_diff <- function(numbers){
 return(sd(diff(numbers) ))
}

calc_sd_diff(numbers.obs)

NMC <- 10000
results <- 0
for(i in 1:NMC){
  results[i] <- calc_sd_diff(generate_numbers(20))
}
hist(results)

mean(results <= calc_sd_diff(numbers.obs))
```

LEt's try the count of how many differences are positive. LEt's see what that looks like.
```{r}
count_pos_diff <- function(numbers){
 return(sum(diff(numbers)>0 ))
}

count_pos_diff(numbers.obs)

NMC <- 10000
results <- 0
for(i in 1:NMC){
  results[i] <- count_pos_diff(generate_numbers(20))
}
hist(results)

mean(results <= count_pos_diff(numbers.obs))
```





```{r}

# we also need some statistic or measurement of our data which 
# indicates the weirdness of it.
# If we take the differences of each consecutive pair of numbers,
# we can look at the standard deviation of those differences.
# That might be a good statistic to use.

calculate_sd_of_diff <- function(data){
  return(sd(diff(data)))
}
```

With these in mind, let's look at a distribution of what this test statistic would look like for random string of numbers

```{r}
results <- 0
NMC <- 10000
for(i in 1:NMC){
  results[i] <- calculate_sd_of_diff(generate_numbers(20))
}
hist(results)
```

```{r}
obs.data <- c(1, 6, 18, 12, 15, 19, 20, 5, 2, 17, 11, 3, 9, 16, 4, 7, 8, 10, 13, 14)
calculate_sd_of_diff(obs.data)
mean(results <= calculate_sd_of_diff(obs.data))
```

This test statistic does not indicate what we observed was particularly unlikely. In fact, we would want to attribute "human-generated" status if the SD was particularly low or particularly high, so this really is a two-tailed test. Thus we'd want to double that $p$-value which gives us roughly 0.30, not particularly rare of an occurrence.






Let's try the count of positive differences as a statistic to see the distribution of. 

```{r}
count_pos_diffs <- function(data){
  return(sum(diff(data)>0))
}
count_pos_diffs(obs.data)
```

We had 13 positive differences. That doesn't seem too high. But maybe that's unusual in a random shuffling. Let's look at the distribution we get from MC simulation.

```{r}
results <- 0
NMC <- 10000
for(i in 1:NMC){
  results[i] <- count_pos_diffs(generate_numbers(20))
}
hist(results)
mean(results >= count_pos_diffs(obs.data))
```
What we got was most likely from humans, not from the computer!

Now it's important to note that if we have too many or too few positive differences that would just as well be evidence of human-sourced numbers. So this is actually a two-tailed test.

What we should do is to double the right-tail p-value.

```{r}
mean(results >= count_pos_diffs(obs.data)) * 2


```
Still pretty small. This is strong evidence that our numbers were generated by a human rather than computer.

What about the string

2  9  3 13 18  4 17 10  1 15  7  5 12 20  6 19  8 16 14 11

Do you think this was generated by computer or by human?
```{r}
new.obs.data<- c(2,9,3,13,18,4,17,10,1,15,7,5,12,20,6,19,8,16,14,11)
min(mean(results >= count_pos_diffs(new.obs.data)),
    mean(results <= count_pos_diffs(new.obs.data))) * 2

```
