---
title: "Homework 13"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem #1. Guided k-fold CV exercise <small>9pts</small>

In this exercise, we will guide you through an exercise where you are asked to use k-fold cross validation to evaluate the performance of several models.

For this exercise we will use the "Swiss Fertility and Socioeconomic Indicators (1888)" dataset from the `datasets` package, which is loaded below. (To view the help page, run `?datasets::swiss` in your console). We will be using `Fertility` as our response variable.

```{r}
swiss = datasets::swiss
```


### Part a) Understanding/visualizing data

Read the help page and briefly "introduce" this dataset. Specifically, explain where the data comes from, what variables it contains, and why should people care about the dataset.

Produce one or some visualizations of the data. Do your best here to try to use your plots to help your viewer best understand the structure and patterns of this dataset. Choose your plots carefully and briefly explain what each plot tells you about the data.

```{r}
#insert code here

```


### Part b) Starting with basic lm

Compare a model with all predictors with no interactions with 2 other models of YOUR choice. Fit all 3 models, show their summary outputs, and briefly comment on which one you think might perform the best when used for future predictions and why.

```{r}
#insert code here

```


### Part c) Estimating MSE using CV

Now, we are going to actually estimate the MSE of each model with K-fold cross validation. First we're going to set a seed and import the `caret` package (it should be already installed since it's a prerequisite for many other packages, but if it's not for some reason, you can install it with `install.packages("caret")`)

```{r}
set.seed(1)
library(caret)
```

Next, use the following chunk, which already has `method` set to `lm`, `data` set to the `swiss` data set, and validation method set to use 5-fold CV, to estimate the MSE of each of your models. All you need to do is add in a formula for your model and repeat for all 3 models you have.

```{r,error=T}
set.seed(1)
#remove this comment when you work on your homework
#model1 = train( *FORMULA GOES HERE* , method="lm", data=swiss, trControl = trainControl(method="cv", number=5))

```

Once you have your models fitted, use `print( )` to show the summary statistics for each model. Report the RMSE for each model, which is the square root of the MSE. Which of these models performs the best? Which performed the worst? Do these results agree with your expectations?

Bonus: repeat the above step, using `trControl = trainControl(method="repeatedcv", number=5, repeats=3)` which repeats each CV analysis 3times and averages out each run to get a more stable estimate of the MSE. Compare the results with the unrepeated MSE estimates. How do they compare?


## Problem #2: More cars!  <small>4pts</small>

This `Auto` dataset, in the `Auto.csv` file contains measurements on over 300 cars. In this problem you will look at the effect of sample size and over-fitting. First load the data.

```{r}
Auto <- read.csv("Auto.csv", stringsAsFactors = TRUE)
```

The dataset contains the response variable `mpg` and 7 predictor variables:

* `cylinders`  - the number of engine cylinders
* `displacement` - engine displacement
* `horsepower` 
* `weight`
* `acceleration`
* `year`
* `origin` - there are Asian, US and European cars; indicator variables have been added to the dataset for European and US cars.

The following function pulls the model formula out of the reg subsets object. It will be used later in the code. Be sure to run this chunk to put the function into the environment.
```{r}
# id: model id
# object: regsubsets object
# data: data used to fit regsubsets
# outcome: outcome variable
get_model_formula <- function(id, object, outcome){
  # get models data
  models <- summary(object)$which[id,-1]
  # Get outcome variable
  #form <- as.formula(object$call[[2]])
  #outcome <- all.vars(form)[1]
  # Get model predictors
  predictors <- names(which(models == TRUE))
  predictors <- paste(predictors, collapse = "+")
  # Build model formula
  as.formula(paste0(outcome, "~", predictors))
}  
```

This function, performs best subset model selection on a dataset with a specified response variable. It returns a list of models - the models with the lowest RSS for each model size from 1 to p. It will be used below. (Note: because of the origin variable a modification was made to only fit models up to size 7, not 8. This is because for a small sample size if there are not Asian, US and European cars the model fit will not work if all variables are included due to linear dependence among predictors)
```{r}
library(leaps)
getModels <- function (dataset, responseVar){
  models <- regsubsets(reformulate(".",responseVar), data = Auto.subset, nvmax = ncol(dataset)-2);
  modelList <- list("formula")
  nModels <- length(summary(models))-1
  for(i in 1:nModels){
    modelList[[i]] <- get_model_formula(i, models, responseVar)
  }
  return(modelList)  
}

```

Now we will run some code to answer the questions below. We will simulate having a small sample of cars to work with and fit the linear model. You will notice that in order to average over errors the entire simulation is performed `NMC=50` times. You may modify this if you wish. The primary line you will modify is where the sample size is set.

```{r, warning=FALSE}
sampleSize <- 15  #You should edit this number

NMC <- 50 #number of replications of this simulation
nFolds <- 5  #k=5 for 5-fold CV
nModels <- 7 # we will look at a maximum model size (# predictors) of 7.

errors <- data.frame('fold' = as.factor(rep(1:nFolds, nModels*NMC)),
                     'rep' = rep(1:NMC, each=nFolds*nModels),
                     'model' = rep(1:nModels, rep(nFolds, nModels)),
                     'mse' = rep(0, nModels*nFolds*NMC))
for(k in 1:NMC){
  Auto.subset <- Auto[sample(nrow(Auto), sampleSize),]
  modelList <- getModels(Auto.subset, "mpg")
  
  #Cross Validation
  folds <- split(sample(1:nrow(Auto.subset)), as.factor(1:nFolds))
  for(i in 1:nFolds){
    validation <- Auto.subset[folds[[i]],]
    training <- Auto.subset[-folds[[i]],]
    for(j in 1:nModels){
      fit <- lm(modelList[[j]], data=training)
      predictions <- predict(fit, newdata = validation)
      errors[errors$rep==k & errors$fold==i  & errors$model ==j, 'mse'] = mean((predictions-validation$mpg)^2)
    }
  }
}
avg <- aggregate(.~model, data=errors, FUN="mean")
plot(y=sqrt(avg$mse), x=avg$model, xlab="model", ylab="root mean square error", type="l", main="Comparison of Model Error")
```

a. If the sample size is 15, what is the size of the preferred model?

b. If the sample size is larger, say 60, what is the size of the preferred model?

c. Now consider if you have a sample of size 200. Does your preferred model change?

d. What is your general conclusion after looking at the effect of sample size on model size and model error?



### Problem #3: Optimal K  <small>8pts; 2pts each</small>

Suppose the variable $Y=4 + 5X_1 + 8X_2 + \epsilon$ where $\epsilon \sim N(0, 2^2)$. Pretend this is the true model, but we don't know that - we are going to collect a random sample of size 40 and fit a linear model. We want to estimate the model error using $K-fold$ cross validation. In this problem we will figure out the optimal number of folds to get the best estimate of the model error $E(Y_{n+1}-\hat{Y}_{n+1})^2$.

We have to make a few assumptions to do this estimation. Let's suppose that $X_1 \sim N(3, 1^2)$ and $X_2 \sim N(1, .5^2)$. You can use the following function to simulate data:

```{r}
simulate.data <- function(n=40){
  X1 <- rnorm(n, 3, 1)
  X2 <- rnorm(n, 1, .5)
  eps <- rnorm(n, 0, 2)
  Y <- 4 + 5*X1 + 8*X2 + eps
  return(data.frame(Y,X1,X2))
}
```

### a. Estimate model error using Monte Carlo

Use Monte Carlo estimation to estimate the MSE of a linear model fit to a sample size of 40 using both predictors. On each MC repetition you should:

  i. generate a sample data set of size 40
  ii. fit a linear model using both X1 and X2 as predictors
  iii. simulate 1000 (or more) out of sample data points
  iv. calculate the average squared error on the out of sample data points.


```{r}
NMC <- 1000
nUnseen <- 1000
Ehat <- 0 #an empty vector to store estimated 

for(i in 1:NMC){
  # generate a sample of size 40

    
  #fit the linear model with X1 and X2 as predictors
  
  
  #simulate unseen data
  
  
  #calculate the average squared error on the out of sample data points
  #store this in Ehat[i]

}
(modelError <- mean(Ehat))
```

### b. Estimating MSE using CV
Now we imagine we don't know the true model error, but instead we want to estimate it with K-fold validation. The following function can be used to perform K-fold validation to estimate mean squared error

```{r, warning=FALSE}
kfoldCV <- function(K, formula, dataset, responseVar){
  #idx is a shuffled vector of row numbers
  idx <- sample(1:nrow(dataset))
  #folds partitions the row indices
  folds <- suppressWarnings(split(idx, as.factor(1:K)))
  #an empty vector to hold estimated errors
  errors <- vector("numeric", K) 
  for(k in 1:K){
    #split the data into training and testing sets
    training <- dataset[-folds[[k]],]
    testing <- dataset[folds[[k]],]
    #go through each model and estimate MSE
    #fit the model to the training data
    fit <- lm(formula = formula, data=training)
    #calculate the sqrt of average squared error on the testing data
    errors[k] <- mean((predict(fit, newdata=testing)-testing[,responseVar])^2)
  }
  return(mean(errors))
}
```

The following code runs an estimation simulation to help you see what happens to the estimate of model error as the number of folds increases. 
We will consider 2,4,5,8,10,20 and 40-fold CV. (these are the factors of 40, so folds will be equally sized).

```{r}
NMC <- 500 ## Change this to 100 if this runs too slow on your machine.
Ks <- c(2,4,5,8,10,20,40)
nK <- length(Ks)
formula <- reformulate(c("X1","X2"),"Y")

errors <- data.frame('replicate'=rep(1:NMC, each=nK),
                     'k' = rep(Ks, NMC),
                     'error' = rep(0, NMC*nK))
for(i in 1:NMC){
  myData <- simulate.data(40)
  for(k in Ks){
    errors[errors$replicate==i & errors$k==k, 'error'] <- kfoldCV(k, formula, myData, 'Y')
  }
}
averageErrors <- aggregate(error ~k, data=errors, FUN="mean")

plot(error ~ k, data=errors, col=rgb(0,0,0,.1))
lines(error ~ k, data=averageErrors, col="red", lwd=3)
abline(h=modelError)
```

From the code and plot generated answer the following questions:

i. What happens to the estimate of model error as the number of folds increases?

ii. Knowing the true model error, what number of folds seems to give the most unbiased estimate of model error?

iii. Besides the estimate being unbiased, what other consideration would you want to make when you consider the number of folds to choose?



### c. The tradeoff
Finally look at this plot:
```{r}
vars <- aggregate(error ~k, data=errors, FUN="var")$error
bias2 <- (averageErrors$error-2.07)^2
errors$errorsq <- (errors$error-2.07)^2
mse <- aggregate(errorsq ~ k, data=errors, FUN="mean")$errorsq

plot(x=Ks, y=vars, ylim=c(0,max(mse)), type="l", ylab="")
lines(x=Ks, y=bias2, lty=2, col="blue")
lines(x=Ks, y=mse, lty=3, lwd=2, col="red")
```

i. What does the solid black line represent? What pattern/trend do you see?

ii. What does the dotted blue line represent? What pattern/trend do you observe?

iii. What does the dotted red line represent? What pattern/trend do you observe?

### d. Wrapping it up
Finally after all of this analysis, what number of folds would you conclude provides the best estimate of model error?





## Problem #4. Variable selection with `Carseats` <small>9pts (4 and 5)</small>

This question should be answered using the `Carseats` dataset from the `ISLR` package. If you do not have it, make sure to install it.

```{r}
library(ISLR)

Carseats = ISLR::Carseats

# you should read the help page by running ?Carseats
# we can also peek at the data frame before using it
str(Carseats)
head(Carseats)
```


### Part a) Visualizing/fitting

First, make some visualizations of the dataset to help set the stage for the rest of the analysis. Try to pick plots to show that are interesting informative.

```{r}
# insert code here

```

Using some variable selection method (stepwise, LASSO, ridge, or just manually comparing a preselected of models using their MSEs), choose a set of predictors to use to predict `Sales`. Try to find the best model that you can that explains the data well and doesn't have useless predictors. Explain the choices you made and show the final model.

```{r}
# insert code here

```


### Part b) Interpreting/assessing model

According to your chosen model, Which predictors appear to be the most important or significant in predicting sales? Provide an interpretation of each coefficient in your model. Be careful: some of the variables in the model are qualitative!

```{r}
# insert code here

```

Estimate the out of sample MSE of your model and check any assumptions you made during your model fitting process. Discuss any potential model violations. How satisfied are you with your final model?

```{r}
# insert code here

```


