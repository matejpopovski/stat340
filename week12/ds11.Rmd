---
title:  "STAT340 Discussion 11: Logistic regression"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(ggplot2)
```

## XKCD comic

<center><a href="https://xkcd.com/1425/"><img id="comic" src="https://imgs.xkcd.com/comics/tasks.png"></a></center>

## Introduction: Spam Filtering

We'll be exploring a dataset consisting of characteristics of 3,921 emails and whether or not each email is classified as Spam. A dataset like this is a good opportunity to practice logistic regression, classifying an email as spam or not spam to essentially create a spam filter.

The `email.csv` data file has been uplaoded [here](https://pastebin.com/raw/ae1Ne7g2). The following code will load it in directly.

```{r}
email <- read.csv("https://pastebin.com/raw/ae1Ne7g2", header=TRUE)
```

Explore the data frame. A description of the variables can be found <a href="https://statpowers.com/dataDesc/email.txt">here</a>.


What proportion of the emails in this dataset are spam?

```{r}
#Code here

```

## A Categorical Predictor

Let's consider a simple model with just a single predictor: whether or not the email contains the word winner. What proportion of emails with the word "winner" are spam? What proportion of emails without the word "winner" are spam?

```{r}
#TODO: Code here
```

Fit a logistic model predicting spam from just this single predictor. Look at the summary output

```{r}
#TODO: Code here
```
Is winner a significant predictor? How do you know? How do you interpret the intercept and coefficient of this model?

*** 

TODO: Discussion here

***

## A Quantitative Predictor

Let's consider a quantitative predictor - the number of characters in the email (in thousands) in the num_char variable. Fit a new logistic model to the data with only num_char as the predictor variable

```{r}
#TODO: Code here
```
Is num_char a significant predictor? How can you interpret the coefficient?

***

TODO: Discussion here

***

An email comes in with 150 characters (150 is .15 in thousands). What is the estimated probability that such an email is spam?

```{r}
#TODO: Code here

```

Another email comes in with 2300 characters. What is the estimated probability that this email is spam?

```{r}
#TODO: Code here

```

*** 

TODO: Interpret the predictions

***

## Building a better model

Now it's time to build another model. Pick at least 5 predictor variables to include in the model. Before you fit the model record your hunch as to which might be the most powerful predictor (right now we're using the term 'power' loosely). Which variables do you think will have positive coefficients, which will have negative coefficients?

***

TODO: Make your 'predictions' about the predictors

***

Fit your logistic model and look at the summary. Were you correct in any of your hunches? Are any of the variables not significant?


```{r}
#TODO: Code here
```

*** 

TODO: Your comments

***

Play around with the model - try adding or removing predictors until you're happy with a model that has somewhere between 5 to 10 predictors. We haven't gone into great detail in class, but you can use AIC as a metric to determine if the one model is statistically "better" than another (when comparing two models we prefer the model with the lower AIC). However, you can just keep adding variables until you have all the variables you want.

## Validating the Model

Now that you've picked a model, we're going to split our data into two parts - a training set and a testing set. The random number seed will ensure that all students have the same training and testing sets. 

```{r}
set.seed(2023)
tr.idx <- sample(nrow(email),size=floor(nrow(email)*.80), replace=FALSE)
email.tr <- email[tr.idx,]
email.ts <- email[-tr.idx,]
```

Using the model you chose previously, fit it to the training dataset and save the logistic model as spam.filter. Check the summary output to see some diagnostics. Is the summary much different than when the model was fit to the full dataset?

```{r}
#TODO: Code goes here

```

***
TODO: Discussion of the summary output and comparison to the model fit to the full dataset
***


There are different ways to validate the model. We'll run a prediction on the testing dataset and perform some diagnostics. The code below should run for you as long as you named your model appropriately. in order to knit, you should remove the eval=FALSE option

```{r, eval=FALSE}
test.pred <- predict(spam.filter, newdata=email.ts, type="response")
threshold <- .318
spam.predict <- as.numeric(test.pred > threshold)

results.table <- table(spam = email.ts$spam, prediction = spam.predict)
results.table

#The following gives your 'hit' rate by row
prop.table(results.table, 1)

#Accuracy is proportion of correct predictions:
(results.table[1,1]+results.table[2,2])/sum(results.table)

#True positive rate is the likelihood a SPAM will be identified
(results.table[2,2])/sum(results.table[2,])

#False positive rate is the likelihood a legit email will be filtered out
(results.table[1,2])/sum(results.table[1,])
```

You can tune your filter by adjusting the threshold. The default in this code is to classify an email as spam if the predicted probability > .5; What happens when you raise that up to .80? Or lower it to .20? What happens to these metrics? How can you tune the threshold to keep the false positive rate at 5%?

***

TODO: summarize the results for different threshold values

***

## Refining your Spam Filter
Ok - if you have time why don't you try to build the ultimate spam filter. All bets are off, use as many predictors as you want. Try putting in some variable transformations if you want, or interactions. Train it on the training set and validate it on the testing set. Can you create a filter that catches many of the spam emails while letting all of the legitimate emails through?

```{r}
#TODO: Code goes here

```

*** 

TODO: Final comments

***
