---
title: "R11: Ridge and Lasso"
author: "Brian Powers"
date: "2023-11-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## K Fold Validation for Ridge Regression

```{r}
kfoldCV.ridge <- function(K, lambdas, dataset){
  m <- length(lambdas)
  
  #idx is a shuffled vector of row numbers
  idx <- sample(1:nrow(dataset))
  #folds partitions the row indices
  folds <- split(idx, as.factor(1:K))

  #an empty data frame to store the results of each validation
  results <- data.frame(fold = rep(1:K, rep(m,K)),
                        model = rep(1:m, K),
                        error = 0)    
  for(k in 1:K){
    #split the data into training and testing sets
    training <- dataset[-folds[[k]],]
    testing <- dataset[folds[[k]],]
    #go through each model and estimate MSE
    ridge_models <- lm.ridge(mpg~0+., cbind(1,training), lambda=lambdas, Inter=TRUE);

    for(f in 1:m){
      coeff <- coef(ridge_models)[f,]
      
      Y <- testing$mpg 
      X <- cbind( 1, testing[,-1])

      Y.hat <- coeff %*% t(X)
            
      #calculate the average squared error on the testing data
      results[results$fold == k & results$model == f, "error"] <- mean((Y-Y.hat)^2)
    }
  }
  #aggregate over each model, averaging the error
  aggregated <- aggregate(error~model, data=results, FUN="mean")
  #produces a simple line & dot plot
  plot(error ~ sqrt(lambdas), type="b", data=aggregated, ylab="MSE")
#  lines(error ~ model, data=aggregated)
  print(which(aggregated$error == min(aggregated$error)))
  print(lambdas[[which(aggregated$error == min(aggregated$error))]])
  return(aggregated)
}

```


## Ridge Regression

```{r}
library(MASS);

lambda_vals <- c(0,.5,1,2,5,10,20,50,100,200,500); # Choose lambdas to try.
# lm.ridge needs:
# 1) a model (mpg~. says to model mpg as an intercept
#         plus a coefficient for every other variable in the data frame)
# 2) a data set (mtcars, of course)
# 3) a value for lambda. lambda=0 is the default,
#         and recovers classic linear regression.
#         But we can also pass a whole vector of lambdas, like we are about to do,
#         and lm.ridge will fit a separate model for each.
# See ?lm.ridge for details.
ridge_models <- lm.ridge(mpg~., mtcars, lambda=lambda_vals);

# Naively plotting this object shows us how the different coefficients
# change as lambda changes.
plot( ridge_models );

kfoldCV.ridge(4, lambda_vals, mtcars)
```

```{r}
ridge_models

```


## LASSO Regression


```{r}
library(glmnet)

kfoldCV.LASSO <- function(K, lambdas, dataset){
  m <- length(lambdas)
  
  #idx is a shuffled vector of row numbers
  idx <- sample(1:nrow(dataset))
  #folds partitions the row indices
  folds <- split(idx, as.factor(1:K))

  #an empty data frame to store the results of each validation
  results <- data.frame(fold = rep(1:K, rep(m,K)),
                        model = rep(1:m, K),
                        error = 0)    
  for(k in 1:K){
    #split the data into training and testing sets
    training <- dataset[-folds[[k]],]
    testing <- dataset[folds[[k]],]
    #go through each model and estimate MSE

      for(f in 1:m){
      mtc_lasso_lambda <- glmnet(training[,-1], training[,1], alpha = 1, lambda=lambdas[f]);
      coeffs <- as.vector(coef(mtc_lasso_lambda))
      y.mtc.predict <- coeffs %*% t(cbind(1,testing[,-1]))

      results[results$fold == k & results$model == f, "error"] <- mean((y.mtc.predict-testing[,1])^2)
    }
  }
  #aggregate over each model, averaging the error
  aggregated <- aggregate(error~model, data=results, FUN="mean")
  #produces a simple line & dot plot
  plot(error ~ lambdas, type="b", data=aggregated, ylab="MSE")
#  lines(error ~ model, data=aggregated)
  print(which(aggregated$error == min(aggregated$error)))
  print(lambdas[[which(aggregated$error == min(aggregated$error))]])
  return(aggregated)
}

lambda_vals <- c(0,.1,.2,.3,.5,.7, 1, 1.5, 2, 2.5, 3, 3.5, 4, 5, 7, 10)
kfoldCV.LASSO(4, lambda_vals, mtcars)
LASSO.fits <- glmnet(mtcars[,-1], mtcars[,1], alpha=1, lambda=lambda_vals)
plot(LASSO.fits, label=TRUE, xvar="lambda")

LASSO_coeff<- t(coef(LASSO.fits))
```

```{r}



```