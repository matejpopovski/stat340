---
title:  "STAT340: Discussion 10: linear regression, continued"
output: html_document
author: "Matej Popovski, Alex Holland, Brock Lunstrum, Zoe Weinstein"
---

## XKCD comic

<center><a href="https://xkcd.com/1725/"><img id="comic" src="https://imgs.xkcd.com/comics/linear_regression.png"></a></center>

---

Today, we'll continue our discussion of linear regression with a pair of exercises.
The first will illustrate how including nonlinearities in our model can improve prediction
The second will give you some practice with  multiple regression and feature selection. 

## Part 1) Nonlinearity and the `cars` Data Set

The `cars` data set (note: this is distinct from the `mtcars` data set!) contains data on stopping distances for cars driving at different speeds.
```{r}
data(cars)
head(cars)
```

As you can see, the data set has just two columns: `speed` and `dist`, corresponding to speed (in miles per hour) and stopping distance (in feet), respectively.
Note that this data was gathered in the 1920s. Modern cars can go a lot faster and stop far more effectively!

__Part a: plotting the data__

Create a scatter plot of the data, showing stopping distance as a function of speed (i.e., distance on the y-axis and speed on the x-axis).
Do you notice a trend?
If so, does it appear at least vaguely linear? Discuss (a sentence or two is plenty).

```{r}

#TODO: code goes here.
# Load the 'cars' dataset
data(cars)

# Scatter plot: stopping distance (y) vs. speed (x)
plot(cars$speed, cars$dist, 
     main = "Stopping Distance vs Speed", 
     xlab = "Speed (mph)", 
     ylab = "Stopping Distance (ft)", 
     pch = 19, col = "blue")



```

***

TODO: discuss here.

The scatter plot shows a clear positive trend: as speed increases, the stopping distance also increases. While the relationship is generally increasing, it does not appear perfectly linear, especially at higher speeds, where the increase in stopping distance seems to become steeper. This suggests a potential curvature, indicating that a nonlinear model might better describe the data.

***

__Part b: fitting linear regression__

Use `lm` to fit a linear regression model that predicts stopping distance from speed (and an intercept term).
That is, fit a model like `dist = beta0 + beta1*speed`.

```{r}

#TODO: code goes here.
# Fit a linear regression model
model <- lm(dist ~ speed, data = cars)

# Display the summary of the model
summary(model)



```

Use the resulting slope and intercept terms to create the scatter plot from Part a, but this time add a line, __in blue__, indicating our fitted model (i.e., add a line with slope and intercept given by your estimated coefficients).

```{r}

# TODO: code goes here.
# Scatter plot of the data
plot(cars$speed, cars$dist, 
     main = "Stopping Distance vs Speed with Fitted Line", 
     xlab = "Speed (mph)", 
     ylab = "Stopping Distance (ft)", 
     pch = 19, col = "blue")

# Add the fitted line (using the coefficients from the model)
abline(model, col = "blue")


```

Do you notice anything about your model?
Is the model a good fit for the data?
Why or why not?
Try looking at the residuals (using both the residuals plot and the Q-Q plot).
Do you notice anything concerning?
Two or three sentences is plenty here.

```{r}

#TODO: code goes here.
# Plotting the residuals
plot(model$residuals, 
     main = "Residuals Plot", 
     xlab = "Index", 
     ylab = "Residuals", 
     pch = 19, col = "red")

# Q-Q plot to check normality of residuals
qqnorm(model$residuals, main = "Q-Q Plot of Residuals")
qqline(model$residuals, col = "blue")


```

***

TODO: discussion goes here.

The linear regression model appears to fit the data reasonably well. The Q-Q plot shows that the residuals are approximately normally distributed, with only a small outlier at the end, which is not a significant concern. The residuals plot indicates that the residuals are generally evenly distributed around zero, which suggests that the model's predictions are unbiased. However, there is a slight increase in variance on the right side, which may indicate some heteroscedasticity. This suggests that while the linear model works reasonably well, there could be room for improvement by considering a more complex or nonlinear model to better capture the increasing variance at higher speeds.

***

Examine the output produced by `lm`.
Interpret the coefficient of `speed`-- what can we conclude from it?
Should we or should we not reject the null hypothesis that the `speed` variable has a non-zero coefficient?

```{r}

#TODO: Code goes here
# View the summary of the linear regression model
summary(model)

```

***

TODO: discussion goes here.

The linear regression output reveals that the relationship between speed and stopping distance is statistically significant, with a p-value for the speed coefficient of 1.49e-12, which is well below the commonly used threshold of 0.05. This strongly suggests that speed has a non-zero effect on stopping distance, and we can confidently reject the null hypothesis that the speed coefficient is zero. The estimated coefficient for speed is 3.9324, indicating that for each additional mile per hour of speed, the stopping distance is expected to increase by about 3.93 feet. Additionally, the model explains approximately 65% of the variability in stopping distance, as indicated by the R-squared value of 0.6511, suggesting a relatively good fit. The residuals appear reasonably well-distributed with some variance at higher speeds, but overall, the model appears to be effective in capturing the relationship between the two variables.

***

__Part c: accounting for nonlinearity__

Let's see if we can improve our model.
We know from physics class that kinetic energy grows like the square of the speed.
Since stopping amounts to getting rid of kinetic energy, it stands to reason that stopping distance might be better predicted by the square of the speed, rather than the speed itself.
It's not exactly clear in the data that such a trend exists, but let's try fitting a different model and see what happens.

Fit the model `dist = beta0 + beta1*speed^2` to the `cars` data. Remember you need to use the I() function in R to transform a predictor in the model.

```{r}

# TODO: code goes here.

# Fit the model where stopping distance is predicted by speed^2
model_squared <- lm(dist ~ I(speed^2), data = cars)

# View the summary of the new model
summary(model_squared)



```

Plot stopping distance as a function of speed again and again add the regression line __in blue__ from Part c.
Then add another line (a curve, really, I guess), __in red__, indicating the prediction of this new model.
That is, the predicted distance as a linear function of *squared* speed.

__Hint:__ the speed values in the data range from 4 to 25. You may find it useful to create a vector `x` containing a sequence of appropriately-spaced points from 4 to 25 and evaluate your model at those `x` values.

__Another hint:__ this is the rare problem where it's probably actually easier to use `ggplot2`, but if you prefer to do everything in R, don't forget about the `lines` function, which might be helpful here.

```{r}

# TODO: code goes here.
# Load the required library
library(ggplot2)

# Create a sequence of x values (speed) from 4 to 25
x <- seq(4, 25, length.out = 100)

# Predict stopping distance using the original linear model (from Part b)
y_linear <- predict(model, newdata = data.frame(speed = x))

# Predict stopping distance using the squared speed model (from Part c)
y_squared <- predict(model_squared, newdata = data.frame(speed = x))

# Create a data frame for the predicted values (for the lines)
pred_data <- data.frame(speed = x, y_linear = y_linear, y_squared = y_squared)

# Create the scatter plot and add both regression lines
ggplot(cars, aes(x = speed, y = dist)) +
  geom_point() + # scatter plot of the data
  geom_line(data = pred_data, aes(x = speed, y = y_linear), color = "blue") + # blue line for original linear model
  geom_line(data = pred_data, aes(x = speed, y = y_squared), color = "red") + # red line for squared speed model
  labs(title = "Stopping Distance as a Function of Speed",
       x = "Speed (mph)",
       y = "Stopping Distance (ft)") +
  theme_minimal()



```

__Part d: which is better?__

Compare the linear and quadratic models fitted above.
Which one describes the data better?
What do you base that claim on?
__Hint:__ consider comparing things like RSE, $R^2$ and comparing the residuals.
A couple of sentences is plenty.

```{r}

#TODO: code goes here.
# Compare the two models
summary(model)          # Linear model
summary(model_squared)  # Quadratic model

# Residual Standard Error (RSE)
rse_linear <- summary(model)$sigma
rse_squared <- summary(model_squared)$sigma

# R-squared values
r2_linear <- summary(model)$r.squared
r2_squared <- summary(model_squared)$r.squared

# Compare residuals visually
par(mfrow = c(1, 2))
plot(residuals(model), main = "Residuals - Linear Model", ylab = "Residuals", xlab = "Index")
abline(h = 0, col = "red")
plot(residuals(model_squared), main = "Residuals - Quadratic Model", ylab = "Residuals", xlab = "Index")
abline(h = 0, col = "red")

# Output the comparison
list(
  RSE_linear = rse_linear,
  RSE_squared = rse_squared,
  R2_linear = r2_linear,
  R2_squared = r2_squared
)


```

***

Comparing the RSE of these two models, the non-linear model achieves an ever so slightly better reconstruction of the responses.
The residuals in the quadratic model display far more homoscedasticity than the linear model.

***

## Multiple regression: a preview of feature selection

Let's return yet again to the `mtcars` data set that we've discussed in lecture.
Recall that the columns of this data set are
```{r}
names(mtcars)
```

Suppose that our goal is to predict the miles per gallon (`mpg`) of cars using the other predictors.
This exercise will get you some practice working with simple and multiple linear regression, and will preview some ideas that we will revisit in a few weeks when we discuss model selection.

### Part a) comparing predictors

Pick three of those predictors.
For each one, fit a model of the form `y ~ 1 + x`.
That is, for each of your predictors, fit a simple linear regression model that predicts `mpg` from *just* that predictor (and an intercept term).

Look at the RSE for each of these three models.
Which one fits the data best?

```{r}

#TODO: code goes here for model 1
model1 <- lm(mpg ~ hp, data = mtcars)
summary(model1)

```

```{r}

#TODO: code goes here for model 2
model2 <- lm(mpg ~ wt, data = mtcars)
summary(model2)

```

```{r}

#TODO: code goes here for model 3
model3 <- lm(mpg ~ qsec, data = mtcars)
summary(model3)

```

 ***
 
TODO: brief discussion goes here.

When comparing the three simple linear regression models for predicting mpg, we see distinct differences in how well each predictor explains the variance in the data. The hp (horsepower) model has a moderate R-squared value of 0.6024, suggesting a fair fit, but the wt (weight) model significantly outperforms it with an R-squared of 0.7528 and a smaller residual standard error of 3.046. This indicates that weight is a stronger predictor for miles per gallon than horsepower. On the other hand, the qsec (quarter-mile time) model provides the weakest fit, with an R-squared of 0.1753 and a higher residual standard error of 5.564, highlighting that it is a much less effective predictor of mpg. Overall, Model 2, which uses weight (wt), offers the best fit to the data and should be considered the most reliable predictor of miles per gallon.
 
 ***

### Part b) Combining features

Now, from your three predictors in part (a) above, consider the three possible ways of choosing two of these three predictors.
For each such pair, fit a multiple-regression model that predicts `mpg` using those two predictors.
Then, compare the RSEs of those three models.
Which does best?
Does the best model include the predictor that did best in Part (a)?
Are both predictors significant in all three models?

```{r}

#TODO: code goes here for model 1
model1 <- lm(mpg ~ hp + wt, data = mtcars)
summary(model1)

```

```{r}

#TODO: code goes here for model 2
model2 <- lm(mpg ~ hp + qsec, data = mtcars)
summary(model2)

```

```{r}

#TODO: code goes here for model 3
model3 <- lm(mpg ~ wt + qsec, data = mtcars)
summary(model3)

```

### Part c) Looking ahead

Now, compare the performance of your two-predictor models in part (b) to the performance of your models in part (c).
Unless something really weird happened, you should see that the two-predictor models outperform the single-variable models as measured by things like RSE and $R^2$.

So we can trivially improve our model's accuracy by adding more predictors-- after all, more predictors will give our model more information to work with (indeed, we can make this far more precise with linear algebra, but that's for another class)!
Verify this fact by fitting a multiple regression model on all three predictors in your models from Parts (a) and (b).
Compare its RSE and $R^2$ to those of the models in Part (b).
You should see that this three-predictor model improves upon all three models in Part (b).

```{r}

#TODO: code goes here.
# Multiple regression model with all three predictors: hp, wt, qsec
model_all <- lm(mpg ~ hp + wt + qsec, data = mtcars)

# Summary of the model to see RSE and R-squared
summary(model_all)


```

So adding more predictors will always improve our model.
On the other hand, it doesn't seem quite fair-- adding more predictors to our model will *always* improve the reconstruction accuracy of our model (again, as measured by RSE or $R^2$, etc).
Should we prefer a model with one predictor over a model with two predictors that has only slightly better prediction accuracy?
How do we know when to stop adding predictors?
That's the problem of *feature selection*, which we'll come back to in a few weeks.

Use diagonstics within R to assess the regression assumptions on the model with 3 predictors. Are the residuals normally distributed? Is the variance of residuals independent of the predicted mpg value? Is the linearity assumption valid (is there a nonlinear pattern to the residuals)? Are there any extreme outliers among the residuals? 

If any of the assumptions are violated, see if you can modify the model to correct this. Consider variable transformations.

```{r}

#TODO: code goes here.
# Fit the model with three predictors
model_all <- lm(mpg ~ hp + wt + qsec, data = mtcars)

# Diagnostic plots
par(mfrow = c(2, 2))  # Set up a 2x2 plotting layout

# Residuals vs Fitted values (for homoscedasticity and linearity)
plot(model_all, which = 1)

# Normal Q-Q plot (for normality of residuals)
plot(model_all, which = 2)

# Scale-Location plot (another test for homoscedasticity)
plot(model_all, which = 3)

# Residuals vs Leverage (to detect outliers)
plot(model_all, which = 5)


```

 ***
 
TODO: brief discussion goes here.

The diagnostic plots reveal several important issues with the regression model. The Residuals vs. Fitted plot shows a slight curve, indicating potential non-linearity in the relationship between the predictors and the response variable, mpg. The Q-Q plot suggests the residuals deviate from normality, especially on the right side, indicating that the normality assumption may be violated. The Scale-Location plot shows a possible increase in the spread of residuals as the fitted values increase, suggesting heteroscedasticity (non-constant variance). Finally, the Residuals vs. Leverage plot highlights the presence of influential points, such as "Chrysler Imperial" and "Maserati Bora," which have high leverage and may disproportionately affect the model. These diagnostics suggest that transformations or alternative methods may be needed to address the violations of assumptions, such as considering polynomial terms for non-linearity, applying a transformation to the dependent variable to address heteroscedasticity, and investigating the influence of outliers.
 
 ***


