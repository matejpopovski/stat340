---
title: "Homework 10"
author: "Matej Popovski"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=T,eval=T,message=F,warning=F,fig.align='center')
library(tidyverse)
```

## Problem 1) More regression with `mtcars` (6 points, 1pt each)

In lecture, we worked briefly with the `mtcars` data set.
Let's get more regression practice by working with it some more.

### a) background

Run `?mtcars` in the console (please __do not__ add it to this `Rmd` file) and briefly read the help page.
Specifically, take note of the following:

1. What is the source of this data?
> The dataset was extracted from the 1974 issue of Motor Trend US Magazine. It documents the results of road tests for various automobiles from the 1973–1974 model years.

2. What is this data set measuring (i.e., what was the response variable in the original study, at least based on the brief description in the R documentation)?
> The primary response variable is miles per gallon (mpg), which represents fuel consumption efficiency.

3. What predictors are available and what do they mean?
> The dataset contains 10 predictors that describe various aspects of automobile design and performance: cyl (number of cylinders, typically 4, 6, or 8), disp (engine displacement in cubic inches), hp (gross horsepower), drat (rear axle ratio), wt (weight of the car in 1000 lbs), qsec (time to cover a 1/4 mile in seconds), vs (engine type, 0 = V-shaped, 1 = straight), am (transmission type, 0 = automatic, 1 = manual), gear (number of forward gears, usually 3, 4, or 5), and carb (number of carburetors, typically ranging from 1 to 8).

You may want to also run `head(mtcars, 10)` or `View(mtcars)` to inspect the data frame briefly before moving on.
```{r}
head(mtcars, 10)
```


### b) Fitting a model

Use `lm` to run a regression of `mpg` on a few predictors in the data frame (choose two or three that you think would make a good model -- don't use all ten; we'll talk about why in later lectures). Save your fitted model as an object called `lm.mtcars`.
Make sure to include `data = mtcars` as a keyword argument to `lm` so that R knows what data frame to use.

Briefly inspect the residuals plot by running `plot(lm.mtcars,ask=F,which=1:2)`.
What do you observe, and what does it mean?

```{r}
# Fit a regression model with mpg as the response variable
lm.mtcars <- lm(mpg ~ wt + hp + qsec, data = mtcars)

# Inspect residual plots
plot(lm.mtcars, ask = FALSE, which = 1:2)

```

> The residuals show randomness in the 'residuals vs fitted' plot and align with the line in the Q-Q plot, the model assumptions are likely valid. In the 'residuals vs fitted' plot for small and big fitted values, I can see that residuals are more positive than negative. Same is shown in the Q-Q plot.


```{r}
lm.mtcars <- lm(mpg ~ wt + hp + drat, data = mtcars)

plot(lm.mtcars,ask=F,which=1:2)
```

> This is another example, where we have slightly worse fitting in the left and right tails.

### c) Interpreting the model

View the summary of your model by uncommenting and running the code below.
```{r}
lm.mtcars <- lm(mpg ~ wt + hp + qsec, data = mtcars)
summary(lm.mtcars)
```

Pick one of your predictors and give an interpretation of the estimate and standard error for its coefficient.
Be careful in your wording of the interpretation.

Which coefficients are statistically significantly different from zero? How do you know?

> The coefficient for wt (weight) in the regression model has an estimate of -4.35880. This indicates that for every additional 1000 lbs of car weight, the expected miles per gallon (mpg) decreases by approximately 4.36 units, assuming the other predictors (hp and qsec) remain constant. The associated standard error for this coefficient is 0.75270, which measures the variability in the estimate. A smaller standard error, suggests greater precision in the estimate and increases confidence in the effect of weight on mpg.

> In this model, the p-value for wt is 3.22e-06, which is well below 0.05, making it statistically significant. On the other hand, the p-values for hp (0.24418) and qsec (0.25463) are both above 0.05, indicating that these predictors are not statistically significantly different from zero and may not have a meaningful impact on mpg in this model.

> In summary, among the three predictors, only wt shows a statistically significant relationship with mpg, providing strong evidence that car weight is an important factor in determining fuel efficiency. The lack of significance for hp and qsec suggests these predictors may not contribute as effectively to explaining variations in mpg.


### d) Interpreting residuals

What is the Residual Standard Error (RSE) for this model? How many degrees of freedom does it have?

> The Residual Standard Error (RSE) for this model is 2.578. RSE measures the average deviation of the observed mpg values from the predicted values by the model. A smaller RSE indicates that the model's predictions are closer to the actual data points. In this case, the RSE suggests that the model's predictions typically deviate from the true mpg values by approximately 2.578 miles per gallon.

> The model has 28 degrees of freedom, which corresponds to the number of observations (32) minus the number of fitted parameters (3 predictors plus the intercept, making 4 parameters). Degrees of freedom reflect the amount of information available for estimating variability.

What is the value of $R^2$ for this model? (__Hint:__ look at the output of `summary`) Give an interpretation of this value.

The multiple R-squared value close to 1 suggest a good fit. Value of 0.8348 indicates that approximately 83.48% of the variability in the response variable mpg is explained by the predictors wt, hp and qsec. This means the model fits the data quite well.
It always increases when more predictors are added to the model, regardless of whether those predictors are relevant. So it can be optimistic especially with many predictors.

### e) Adjusted $R^2$

Briefly read about the adjusted $R^2$ [here](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/adjusted-r2/).
What is the adjusted $R^2$ of this model and how does this differ from the usual $R^2$ value? (__Hint:__ again, look at the output of `summary`).

> R-squared has a tendency to increase as more predictors are added to the model, even if those predictors are not truly meaningful.

> The adjusted R^2 value is a modified version of the regular R^2 value that accounts for the number of predictors in a model. It includes a penalty for adding additional predictors to the model. It increases only if the new predictor improves the model more than would be expected by chance and decreases when the predictor does not add value, giving more realistic estimate of goodness-of-fit.

> adjusted R^2 = 1 - ((1-R^2)(n-1)/(n-k-1))

> n is number of observations and k is number of predictors.

> In our case adjusted R-squared = 0.8171 suggests that predictors hp and qsec do not contribute to explanation of variance. Thus, the adjusted R-squared (0.8171) is slightly lower than the regular R-squared (0.8348), suggesting that after adjusting for the number of predictors, the model explains about 81.71% of the variance in mpg. This makes the adjusted R-squared a more reliable measure of the model’s goodness of fit, especially when comparing models with different numbers of predictors.


### f) CIs for coefficients

Read the documentation for the `confint` function, and use it to generate $95\%$ confidence intervals for the coefficients of your model.
Give an interpretation of these confidence intervals.

```{r}
lm.mtcars <- lm(mpg ~ wt + hp + qsec, data = mtcars)
confint(lm.mtcars, level = 0.95)
```

The confidence intervals provide valuable information about the precision and significance of the model coefficients. Narrower intervals indicate more precise estimates, while intervals that do not include zero indicate statistically significant predictors.
CI for predictor wt do not include zero and are negative. This means that wt is (also seen by p-value) significantly associated with mpg, and the relationship is negative. As weight increases, mpg decreases. Hp and qsec include zero, and do not significantly contribute to mpg.


## Problem 2) the `cats` data set (8 points; 2pt each)

The `cats` data set, included in the `MASS` library, contains data recorded from 144 cats.
Each row of the data set contains the body weight (`Bwt`, in kgs), heart weight (`Hwt`, in grams) and the sex (`Sex`, levels `'F'` and `'M'`) for one of the cats in the data set.

```{r}
library(MASS)
head(cats)
```

### a) plotting the data

Create a scatter plot showing heart weight on the y-axis and body weight on the x-axis.
Ignore the `Sex` variable in this plot.

Briefly describe what you see. Is there a clear trend in the data?

```{r}
# Load the MASS library
library(MASS)

# Extract the cats dataset
data(cats)

# Create a scatterplot of Bwt vs Hwt
plot(cats$Bwt, cats$Hwt,
     xlab = "Body Weight (kg)",
     ylab = "Heart Weight (g)",
     main = "Scatterplot of Body Weight vs Heart Weight",
     pch = 19, col = "blue")

# Add a trend line
abline(lm(Hwt ~ Bwt, data = cats), col = "red")

```

> A trend showing the linear relationship between the two attributes.

### b) fitting a linear model

Fit a linear regression model to predict cat heart weight from cat body weight (and using an intercept term, of course).

Examine the coefficients of your fitted model.
What is the coefficient for the `Bwt` variable?
Interpret this coefficient-- a unit change in body weight yields how much change in heart weight?

```{r}
# Fit a linear model predicting Hwt from Bwt
lm.cats <- lm(Hwt ~ Bwt, data = cats)

# View the summary of the fitted model
summary(lm.cats)

```

> Estimated Bwt coefficient  is 4.0341. For each unit increase in body weight (Bwt), the heart weight (Hwt) increases by approximately 4.034 grams, on average.

### c) back to plotting

Create the same plot from Part a above, but this time color the points in the scatter plot according to the `Sex` variable.
You may use either `ggplot2` or the built-in R plotting tools, though I would recommend the former, for this.

You should see a clear pattern. Describe it. A sentence or two is fine here.

```{r}
# Load the MASS library
library(MASS)

# Extract the cats dataset
data(cats)

# Create a scatterplot of Bwt vs Hwt with points colored by Sex
plot(cats$Bwt, cats$Hwt,
     xlab = "Body Weight (kg)",
     ylab = "Heart Weight (g)",
     main = "Scatterplot of Body Weight vs Heart Weight",
     pch = 19, col = ifelse(cats$Sex == "M", "blue", "pink"))

# Add a legend
legend("topright", legend = c("Male", "Female"), col = c("blue", "pink"), pch = 19)

```

Female cats are with lower Body weight range than male cats and they have smaller estimate (slope). This means that for each unit increase in body weight for femalse cats, there will be lower increase of the heart weight (Hwt) comparing with male cats. 


### d) adding `Sex` and an interaction

From looking at the data, it should be clear that the `Sex` variable has explanatory power in predicting heart weight, but it is also very correlated with body weight.

Fit a new linear regression model, still predicting heart weight, but this time including both body weight and sex as predictors *and* an interaction term between body weight and sex.
Take note of how R assigns `Sex` a dummy encoding.

Examine the outputs of your model.
In particular, note the coefficients of `Sex` and the interaction between `Bwt` and `Sex`.
Are both of these coefficients statistically significantly different from zero?
How do you interpret the interaction term?

```{r}
# Fit the linear regression model with an interaction term between Bwt and Sex
lm.cats <- lm(Hwt ~ Bwt * Sex, data = cats)

# View the summary of the model to inspect coefficients
summary(lm.cats)

```
> R assigns Sex a dummy encoding by treating the female category as the reference group (with a coefficient of zero) and encoding the male category as SexM, which reflects the difference in heart weight for male cats compared to female cats.

> Both the coefficient for SexM (male cats) and the interaction term between Bwt and Sex are statistically significant at the 5% level, with p-values of 0.045 and 0.047, respectively. The coefficient for SexM indicates that male cats have a heart weight that is, on average, 4.1654 grams less than female cats, after adjusting for body weight. The interaction term (Bwt:SexM) suggests that the relationship between body weight (Bwt) and heart weight is different for male and female cats. Specifically, for male cats, the effect of body weight on heart weight is 1.6763 grams more per kilogram of body weight than for female cats. This implies that while both male and female cats show a positive relationship between body weight and heart weight, the effect is stronger in male cats.

> All three predictors are significant.


## Problem 3) Quantitative and Categorical Predictors (8 points)

(This problem is based on a plot from [The behavior of different clays subjected to a fast-drying cycle for traditional ceramic manufacturing](https://doi.org/10.1016/j.jksues.2022.05.003), but the data has been simulated)

An experiment was conducted on three types of **material**: Plastic clay, Sandy clay and claystone. The **plastic deformation** (\%) was measured at various **moisture** (\%) levels. Data is found in the `clay_sample.csv` file. Plastic deformation is denoted `pd` in the dataset.

### a) Fit the model (4 points)
Create a linear model model predicting plastic deformation from moisture. Include material and the interaction between material and moisture. 

```{r}
# Load necessary library
library(readr)

# Read the CSV file
clay_sample <- read_csv("clay_sample.csv", show_col_types = FALSE)
head(clay_sample)
```

a.i. Output the standard model summary using `summary()`. 
  
```{r}
lm.clay <- lm(pd ~   moisture * material, data = clay_sample)

summary(lm.clay)
```

  
a.ii. Interpret the $R^2$$ statistic value in one sentence.

> The R-squared statistic represents the proportion of the variance in plastic deformation (pd) that is explained by the model, including moisture levels and material type, and their interaction, with a higher R-squared (87.76%) indicating a better fit of the model to the data.

a.iii. Which material represents the baseline?

> Considering summary results material "claystone" is baseline. The coefficients for "sandy clay" and "plastic clay" are compared to the baseline ("claystone"). These coefficients indicate the difference in plastic deformation (pd) when the material is "sandy clay" or "plastic clay" compared to "claystone".

a.iv. Explain how the residual standard error degrees of freedom is related to sample size and the model size.

> degrees of freedom = 64   calculated by n - (p+1) = 70 - (5+1)
   n = 70 rows, 5 parameters + 1 intercept

```{r}
dim(clay_sample)


```

Call:
lm(formula = pd ~ moisture * material, data = clay_sample)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.2343 -2.9072 -0.4121  2.6426  7.8457 

Coefficients:
                               Estimate Std. Error t value Pr(>|t|)    
(Intercept)                   -103.1666    11.6696  -8.841 1.07e-12 ***
moisture                         7.7928     0.7454  10.455 1.77e-15 ***
materialplastic clay            23.9557    13.2986   1.801 0.076355 .  
materialsandy clay              29.2029    17.7493   1.645 0.104810    
moisture:materialplastic clay   -3.5950     0.7907  -4.547 2.48e-05 ***
moisture:materialsandy clay     -3.5036     0.9388  -3.732 0.000406 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 4.122 on 64 degrees of freedom
Multiple R-squared:  0.8776,	Adjusted R-squared:  0.868 
F-statistic: 91.77 on 5 and 64 DF,  p-value: < 2.2e-16

### b) Interpret coefficient 1 (1 pt)
Provide an interpretation for the value of the coefficient of moisture, and comment on its significance.

> Moisture estimate =  7.7928 and it is highly significant. P-value = 1.77e-15 < 0.05

### c) Interpret coefficient 2 (1 pt)
Provide an interpretation for the value of the coefficient of one of the materials, and comment on its significance.

> Material plastic clay estimate is 23.9557 Std. Eror is 13.2986 and p-value 0.076355.
Plastic Clay has a 23.9557 unit bigger plastic deformation compared to the baseline material (Claystone), holding all other variables fixed.
The p-value for Material plastic clay estimate 0.076355 is greater than 0.05, indicating that the difference in plastic deformation between Plastic Clay and Claystone is not statistically significant.

### d) Interpret coefficient 3 (1 pt)
Provide an interpretation for the value of the coefficient of one of the interaction terms, and comment on its significance.

> The coefficient for the interaction term "moisture clay" is -3.5950, which means that for "plastic clay," the effect of moisture on plastic deformation is 3.595% less than the effect of moisture on the baseline material (claystone). Specifically, as moisture increases, the plastic deformation for "plastic clay" increases at a slower rate compared to claystone. This interaction term is statistically significant with a p-value of 2.48e-05, indicating that the difference in moisture's effect between "plastic clay" and "claystone" is highly significant.

### e) Assess assumptions (1 pt)
Finally display the first two diagnostic plots using `plot(<YOUR MODEL>, which=1:2)` and discuss three of the assumptions of the linear model: normality of error term, linearity and the relationship, and constant variance of the error term. You do not have to do any more modeling after this step, but comment about what you *might* want to do to improve the model.

```{r}

plot(lm.clay, which = 1:2)

```

Diagnostic Plots and Assumptions
Linearity:
Our assumption is the relationship between the predictors and the response variable is linear.
The Residuals vs Fitted plot do not show a random scatter around the horizontal line (zero residuals) especially for the lower values of fited values. 

Normality of Errors:
Our assumption is that residuals (errors) are normally distributed.
The Normal Q-Q plot show the residuals falling along the reference line but have deviations for lower and bigger values.

Homoscedasticity: Variance of the error term seems like it is not constant.

To achieve some improvement I can consider: transforming the response variable or predictors, add additional or more relevant predictors, or introduce non-linear relationship.


## Problem 4) Using Multiple regression to fit nonlinear data (8 points; 2 pt each)

Open the dataset `multData.csv`. This data set consists of three predictor variables, simply named `X1`, `X2` and `X3`. The response variable is `Y`. In this problem you will explore how to use the multiple regression model to model nonlinear relationships.

```{r}
data <- read.csv("multData.csv")
print("First few rows of the dataset:")
head(data)
```


### a) the first model

First we will explore the relationship between $Y$ and the first two predictors $X1$ and $X2$. Fit the linear model

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$$
Interpret the coefficients of both X1 and X2. 

```{r}
# Fit multiple regression model
model <- lm(Y ~ X1 + X2, data = data)

summary(model)

```
> In aditive model 

> The intercept is 948.53, represents the estimated value of the dependent variable Y, when both predictros X1, X2 are equal to zero.

> Coefficient of the first predictor (-12.6318): The coefficient of the first predictor indicates that for each one-unit increase in the first predictor, the predicted value of the dependent variable decreases by 12.63 units, holding the second predictor constant. The negative sign suggests an inverse relationship between the first predictor and the dependent variable. The p-value (< 2e-16) indicates that this relationship is statistically significant.

> Coefficient of the second predictor (-35.1420): The coefficient of the second predictor suggests that for each one-unit increase in the second predictor, the predicted value of the dependent variable decreases by 35.14 units, holding the first predictor constant. Like the first predictor, the second predictor also has a negative relationship with the dependent variable, and this effect is statistically significant with a p-value of 3.42e-09.


estimate for X1 is -12.6318, with Std 0.2439.
estimate for X2 is -35.1420, with Std 4.8472.

Both coefficients are negative indicated that for one unit increase in the predictors X1 and X2, the predicted value of dependent variable will decrease by 12.6318, 35.1420 respectively, holding the other predictor constant.

Both p values are very small 2e-16 and 3.42e-09 indicating that the effect of X1 and X2  on  Y is highly statistically significant. 
Multiple R-squared is 0.9831 and almost the same Adjusted R-squared:  0.9824, meaning that model explains  around 98%  of variability



### b) Investigating interaction of quantitative predictors

Next introduce an interaction term to the model
$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1\cdot X_2 + \epsilon$$

Fit the model and view the summary output. Has this improved the model fit? Did anything surprising happen to the coefficients? Try to explain what happened.




### c) Introducing the last predictor

Next fit a model that introduces the `X3` variable. 

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1\cdot X_2  + \beta_4 X_3 \epsilon$$
Has the model fit improved? In what way (Justify your answer)? 


### d) Considering higher order terms

Finally explore higher order terms for the X3 variable: Introduce $X3^2$, $X3^3$ etc and determine if any of these higher order terms are justified in the model. Explain your reasoning and present your final model. Look at the diagnostic plots and discuss whether the assumptions of the multiple regression model seem to be justified.



