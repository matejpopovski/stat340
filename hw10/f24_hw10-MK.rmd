---
title: "Homework 10"
author: "your name here"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=T,eval=T,message=F,warning=F,fig.align='center')
library(tidyverse)
```

## Problem 1) More regression with `mtcars` (6 points, 1pt each)

In lecture, we worked briefly with the `mtcars` data set.
Let's get more regression practice by working with it some more.

### a) background

Run `?mtcars` in the console (please __do not__ add it to this `Rmd` file) and briefly read the help page.
Specifically, take note of the following:

1. What is the source of this data?
2. What is this data set measuring (i.e., what was the response variable in the original study, at least based on the brief description in the R documentation)?
3. What predictors are available and what do they mean?

You may want to also run `head(mtcars, 10)` or `View(mtcars)` to inspect the data frame briefly before moving on.

### b) Fitting a model

Use `lm` to run a regression of `mpg` on a few predictors in the data frame (choose two or three that you think would make a good model -- don't use all ten; we'll talk about why in later lectures). Save your fitted model as an object called `lm.mtcars`.
Make sure to include `data = mtcars` as a keyword argument to `lm` so that R knows what data frame to use.

Briefly inspect the residuals plot by running `plot(lm.mtcars,ask=F,which=1:2)`.
What do you observe, and what does it mean?

### c) Interpreting the model

View the summary of your model by uncommenting and running the code below.
```{r}
# summary(lm.mtcars)
```

Pick one of your predictors and give an interpretation of the estimate and standard error for its coefficient.
Be careful in your wording of the interpretation.

Which coefficients are statistically significantly different from zero? How do you know?

### d) Interpreting residuals

What is the Residual Standard Error (RSE) for this model? How many degrees of freedom does it have?

What is the value of $R^2$ for this model? (__Hint:__ look at the output of `summary`) Give an interpretation of this value.

### e) Adjusted $R^2$

Briefly read about the adjusted $R^2$ [here](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/adjusted-r2/).
What is the adjusted $R^2$ of this model and how does this differ from the usual $R^2$ value? (__Hint:__ again, look at the output of `summary`).


### f) CIs for coefficients

Read the documentation for the `confint` function, and use it to generate $95\%$ confidence intervals for the coefficients of your model.
Give an interpretation of these confidence intervals.



## Problem 2) the `cats` data set (8 points; 2pt each)

The `cats` data set, included in the `MASS` library, contains data recorded from 144 cats.
Each row of the data set contains the body weight (`Bwt`, in kgs), heart weight (`Hwt`, in grams) and the sex (`Sex`, levels `'F'` and `'M'`) for one of the cats in the data set.

```{r}
library(MASS)
head(cats)
```

### a) plotting the data

Create a scatter plot showing heart weight on the y-axis and body weight on the x-axis.
Ignore the `Sex` variable in this plot.

Briefly describe what you see. Is there a clear trend in the data?


### b) fitting a linear model

Fit a linear regression model to predict cat heart weight from cat body weight (and using an intercept term, of course).

Examine the coefficients of your fitted model.
What is the coefficient for the `Bwt` variable?
Interpret this coefficient-- a unit change in body weight yields how much change in heart weight?


### c) back to plotting

Create the same plot from Part a above, but this time color the points in the scatter plot according to the `Sex` variable.
You may use either `ggplot2` or the built-in R plotting tools, though I would recommend the former, for this.

You should see a clear pattern. Describe it. A sentence or two is fine here.

### d) adding `Sex` and an interaction

From looking at the data, it should be clear that the `Sex` variable has explanatory power in predicting heart weight, but it is also very correlated with body weight.

Fit a new linear regression model, still predicting heart weight, but this time including both body weight and sex as predictors *and* an interaction term between body weight and sex.
Take note of how R assigns `Sex` a dummy encoding.

Examine the outputs of your model.
In particular, note the coefficients of `Sex` and the interaction between `Bwt` and `Sex`.
Are both of these coefficients statistically significantly different from zero?
How do you interpret the interaction term?



## Problem 3) Quantitative and Categorical Predictors (8 points)

(This problem is based on a plot from [The behavior of different clays subjected to a fast-drying cycle for traditional ceramic manufacturing](https://doi.org/10.1016/j.jksues.2022.05.003), but the data has been simulated)

An experiment was conducted on three types of **material**: Plastic clay, Sandy clay and claystone. The **plastic deformation** (\%) was measured at various **moisture** (\%) levels. Data is found in the `clay_sample.csv` file. Plastic deformation is denoted `pd` in the dataset.

### a) Fit the model (4 points)
Create a linear model model predicting plastic deformation from moisture. Include material and the interaction between material and moisture. 

a.i. Output the standard model summary using `summary()`. 
  
a.ii. Interpret the $R^2$$ statistic value in one sentence.

a.iii. Which material represents the baseline?

a.iv. Explain how the residual standard error degrees of freedom is related to sample size and the model size.

### b) Interpret coefficient 1 (1 pt)
Provide an interpretation for the value of the coefficient of moisture, and comment on its significance.

### c) Interpret coefficient 2 (1 pt)
Provide an interpretation for the value of the coefficient of one of the materials, and comment on its significance.

### d) Interpret coefficient 3 (1 pt)
Provide an interpretation for the value of the coefficient of one of the interaction terms, and comment on its significance.

### e) Assess assumptions (1 pt)
Finally display the first two diagnostic plots using `plot(<YOUR MODEL>, which=1:2)` and discuss three of the assumptions of the linear model: normality of error term, linearity and the relationship, and constant variance of the error term. You do not have to do any more modeling after this step, but comment about what you *might* want to do to improve the model.



## Problem 4) Using Multiple regression to fit nonlinear data (8 points; 2 pt each)

Open the dataset `multData.csv`. This data set consists of three predictor variables, simply named `X1`, `X2` and `X3`. The response variable is `Y`. In this problem you will explore how to use the multiple regression model to model nonlinear relationships.

### a) the first model

First we will explore the relationship between $Y$ and the first two predictors $X1$ and $X2$. Fit the linear model

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$$
Interpret the coefficients of both X1 and X2. 

First we look for linear regresion separately for 
$$Y = \beta_0 + \beta_1 X_1  + \epsilon$$
$$Y = \beta_0 + \beta_2 X_2 + \epsilon$$
```{r}
# Load the CSV file
data <- read.csv("multData.csv")

# Display the data
print("First few rows of the dataset:")
head(data)
#data

model_X1 <- lm(Y ~ X1, data = data)
model_X2 <- lm(Y ~ X2, data = data)

# Scatterplots to explore relationships
par(mfrow = c(1, 2))  # Arrange plots side by side
plot(data$X1, data$Y, main = "Y vs x1", xlab = "X1", ylab = "Y", pch = 19, col = "blue")
plot(data$X2, data$Y, main = "Y vs x2", xlab = "X2", ylab = "Y", pch = 19, col = "green")

# Correlation coefficients
cor_X1_Y <- cor(data$X1, data$Y)
cor_X2_Y <- cor(data$X2, data$Y)
cat("Correlation between Y and X1:", cor_X1_Y, "\n")
cat("Correlation between Y and X2:", cor_X2_Y, "\n")

# Fit simple linear regression models
model_X1 <- lm(Y ~ X1, data = data)
model_X2 <- lm(Y ~ X2, data = data)

# Display model summaries
cat("\nLinear Regression Summary for Y ~ X1:\n")
summary(model_X1)

cat("\nLinear Regression Summary for Y ~ X2:\n")
summary(model_X2)

# Add regression lines to scatterplots
par(mfrow = c(1, 2))  # Reset layout
plot(data$X1, data$Y, main = "Y vs X1 with Regression Line", xlab = "X1", ylab = "Y", pch = 19, col = "blue")
abline(model_X1, col = "red", lwd = 2)
plot(data$X2, data$Y, main = "Y vs X2 with Regression Line", xlab = "X2", ylab = "Y", pch = 19, col = "green")
abline(model_X2, col = "red", lwd = 2)

```
Model with both predictors
$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$$


```{r}
# Fit multiple regression model
model <- lm(Y ~ X1 + X2, data = data)

# Display model summary
cat("Summary of Multiple Regression Model:\n")
summary(model)

# Diagnostic plots for regression assumptions
par(mfrow = c(2, 2))  # Arrange diagnostic plots in a 2x2 grid
plot(model)

# Predicted vs Observed Plot
predicted_Y <- predict(model)
plot(data$Y, predicted_Y, 
     main = "Observed vs Predicted Y",
     Xlab = "Observed Y", 
     Ylab = "Predicted Y", 
     pch = 19, col = "blue")
abline(0, 1, col = "red", lwd = 2)  # Add a 45-degree line for reference

# Check correlation of predictors
correlation_matrix <- cor(data[, c("X1", "X2")])
cat("Correlation Matrix for Predictors:\n")
print(correlation_matrix)

```

In aditive model 
estimate for X1 is -12.6318, with Std 0.2439.
estimate for X2 is -35.1420, with Std 4.8472.
Both p values are very small 2e-16 and 3.42e-09 indicating that the effect of X1 and X2  on  Y is highly statistically significant. 
Multiple R-squared is 0.9831 and almost the same Adjusted R-squared:  0.9824, meaning that model explains  around 98%  of variability

### b) Investigating interaction of quantitative predictors

Next introduce an interaction term to the model
$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1\cdot X_2 + \epsilon$$
```{r}
# Fit model with interaction term
model_interaction <- lm(Y ~ X1 * X2, data = data)  # `X1 * X2` includes both main effects and interaction

# Display model summary
cat("Summary of Interaction Model:\n")
summary(model_interaction)
```

Fit the model and view the summary output. Has this improved the model fit? Did anything surprising happen to the coefficients? Try to explain what happened.

```{r}
# Fit model with interaction term
model_interaction <- lm(Y ~ X1 * X2, data = data)  # `X1 * X2` includes both main effects and interaction

# Display model summary
cat("Summary of Interaction Model:\n")
summary(model_interaction)

# Diagnostic plots
par(mfrow = c(2, 2))  # Arrange plots
plot(model_interaction)

# Compare with additive model
# model_additive <- lm(Y ~ X1 + X2, data = data)
# cat("\nSummary of Additive Model:\n")
# summary(model_additive)

# Compare R-squared
cat("\nComparison of R-squared values:\n")
cat("Additive Model R-squared:", summary(model_additive)$r.squared, "\n")
cat("Interaction Model R-squared:", summary(model_interaction)$r.squared, "\n")

# Predicted vs Observed Plot
predicted_Y_interaction <- predict(model_interaction)
plot(data$Y, predicted_Y_interaction, 
     main = "Observed vs Predicted Y (Interaction Model)",
     Xlab = "Observed Y", 
     Ylab = "Predicted Y", 
     pch = 19, col = "blue")
abline(0, 1, col = "red", lwd = 2)

# Check for significance of interaction term
anova(model_additive, model_interaction)

```

In interaction model 
estimate for X1 is  19.0469 with Std 11.1545
estimate for X2 is 6.7768, with Std 15.4337
and X1:X2 estimate -2.0441 with Std 0.7196
Only p value for X1:X2 estimate with value 0.00669 (< 0.05) is significant. This suggest that effect of X1 on X2 depends and vice versa. Looking on correlation coefficient between X1 and X2 it is 0.00988581 

Multiple R-squared is 0.9857 and almost the same Adjusted R-squared:  0.9847, meaning that model explains  around 98%  of variability

Comparing both models through their R-squared and Adjusted R-squared that are around 98% we cant'see difference.  

Comparing both models estimated Residual standard errors are 10.29 on d.f=47 for additive and  9.589 on d.f=46 for interaction model, that seems small preference for interaction model.








### c) Introducing the last predictor

Next fit a model that introduces the `X3` variable. 

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1\cdot X_2  + \beta_4 X_3 \epsilon$$
Has the model fit improved? In what way (Justify your answer)? 

```{r}
# Fit the model with interaction and X3
model_full <- lm(Y ~ X1 * X2 + X3, data = data)

# Display summary of the model
summary(model_full)

```


### d) Considering higher order terms

Finally explore higher order terms for the X3 variable: Introduce $X3^2$, $X3^3$ etc and determine if any of these higher order terms are justified in the model. Explain your reasoning and present your final model. Look at the diagnostic plots and discuss whether the assumptions of the multiple regression model seem to be justified.


```{r}
# Add higher-order terms for X3
data$X3_squared <- data$X3^2
data$X3_cubed <- data$X3^3

# Fit the base model
model_base <- lm(Y ~ X1 * X2 + X3, data = data)

# Add X3^2
model_X3_squared <- lm(Y ~ X1 * X2 + X3 + X3_squared, data = data)

# Add X3^2 and X3^3
model_X3_cubed <- lm(Y ~ X1 * X2 + X3 + X3_squared + X3_cubed, data = data)

# Compare models
summary(model_base)
summary(model_X3_squared)
summary(model_X3_cubed)

# Use ANOVA to compare nested models
anova(model_base, model_X3_squared)
anova(model_X3_squared, model_X3_cubed)

# Plot diagnostic plots for the best model
par(mfrow = c(2, 2))
plot(model_X3_cubed)

```

